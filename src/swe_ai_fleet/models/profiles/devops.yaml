name: "devops"
model: "Qwen/Qwen2.5-Coder-14B-Instruct"  # vLLM model name
context_window: 32768
temperature: 0.6
max_tokens: 4096
# Alternative models for different backends:
# ollama_model: "qwen2.5-coder:14b"
# llama_model: "qwen3-coder-14b-gguf"

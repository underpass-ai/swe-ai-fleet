"""Use case: Generate proposal using LLM."""

from typing import Any

from ..domain import (
    AgentConfig,
    SystemPrompt,
    TaskPrompt,
    VLLMRequest,
)
from ..domain.ports import IVLLMClient


class GenerateProposal:
    """
    Caso de uso: Generar propuesta usando LLM.
    
    Coordina la construcci贸n de prompts y la llamada al LLM
    para generar una propuesta de soluci贸n.
    """
    
    def __init__(self, config: AgentConfig, llm_client: IVLLMClient):
        """
        Initialize use case.
        
        Args:
            config: Configuraci贸n del agente
            llm_client: Cliente LLM (puerto)
        """
        self.config = config
        self.llm_client = llm_client
    
    async def execute(
        self,
        task: str,
        constraints: dict[str, Any],
        diversity: bool = False,
    ) -> dict[str, Any]:
        """
        Ejecutar generaci贸n de propuesta.
        
        Args:
            task: Task description
            constraints: Task constraints (rubric, requirements, metadata)
            diversity: Whether to increase diversity
            
        Returns:
            Proposal dictionary with content and metadata
        """
        # 1. Build system prompt using domain model
        system_prompt = SystemPrompt.for_role(
            role=str(self.config.role),
            rubric=constraints.get("rubric", ""),
            requirements=constraints.get("requirements", []),
            diversity=diversity,
        ).render()
        
        # 2. Build task prompt using domain model
        task_prompt = TaskPrompt(
            task=task,
            metadata=constraints.get("metadata", {}),
        ).render()
        
        # 3. Adjust temperature for diversity
        temperature = self.config.temperature * 1.3 if diversity else self.config.temperature
        
        # 4. Create vLLM request using domain model
        vllm_request = VLLMRequest.create(
            model=self.config.model,
            system_prompt=system_prompt,
            user_prompt=task_prompt,
            temperature=temperature,
            max_tokens=self.config.max_tokens,
        )
        
        # 5. Call LLM using port (dependency injection)
        vllm_response = await self.llm_client.generate(vllm_request)
        
        # 6. Return as dict (for backward compatibility)
        return vllm_response.to_dict()


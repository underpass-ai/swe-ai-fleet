name: "architect"
model: "databricks/dbrx-instruct"  # vLLM model name
context_window: 128000
temperature: 0.3  # Lower temperature for more precise architecture decisions
max_tokens: 8192
# Alternative models for different backends:
# ollama_model: "dbrx:instruct"
# llama_model: "dbrx-moe-gguf"

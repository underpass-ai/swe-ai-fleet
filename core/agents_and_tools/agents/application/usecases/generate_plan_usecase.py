"""Use case for generating execution plans."""

import json
import logging

from core.agents_and_tools.agents.application.dtos.plan_dto import PlanDTO
from core.agents_and_tools.agents.domain.entities.execution_constraints import ExecutionConstraints
from core.agents_and_tools.agents.domain.ports.llm_client import LLMClientPort
from core.agents_and_tools.agents.infrastructure.mappers.execution_step_mapper import ExecutionStepMapper
from core.agents_and_tools.agents.infrastructure.services.json_response_parser import (
    JSONResponseParser,
)
from core.agents_and_tools.agents.infrastructure.services.prompt_loader import PromptLoader
from core.agents_and_tools.common.domain.entities import AgentCapabilities

logger = logging.getLogger(__name__)


class GeneratePlanUseCase:
    """
    Use case for generating execution plans using LLM.

    This use case contains the business logic for:
    - Building prompts for execution planning
    - Parsing LLM responses into plan structures
    - Role-specific prompt engineering
    """

    def __init__(
        self,
        llm_client: LLMClientPort,
        prompt_loader: PromptLoader,
        json_parser: JSONResponseParser,
        step_mapper: ExecutionStepMapper,
    ):
        """
        Initialize use case with all dependencies (fail-fast).

        Args:
            llm_client: Port for LLM communication (low-level API calls) (required)
            prompt_loader: Prompt loader service (required)
            json_parser: JSON parser service (required)
            step_mapper: Mapper for execution steps (required)

        Note:
            All dependencies must be provided. This ensures full testability.
        """
        if not llm_client:
            raise ValueError("llm_client is required (fail-fast)")
        if not prompt_loader:
            raise ValueError("prompt_loader is required (fail-fast)")
        if not json_parser:
            raise ValueError("json_parser is required (fail-fast)")
        if not step_mapper:
            raise ValueError("step_mapper is required (fail-fast)")

        self.llm_client = llm_client
        self.prompt_loader = prompt_loader
        self.json_parser = json_parser
        self.step_mapper = step_mapper

    async def execute(
        self,
        task: str,
        context: str,
        role: str,
        available_tools: AgentCapabilities,
        constraints: ExecutionConstraints | None = None,
    ) -> PlanDTO:
        """
        Generate execution plan for a task.

        This method contains business logic moved from VLLMClient.generate_plan():
        - Build system prompt with role context
        - Build user prompt with task and context
        - Call llm_client.generate() (low-level LLM call)
        - Parse JSON response
        - Return plan dict

        Args:
            task: Task description
            context: Smart context from Context Service
            role: Agent role (DEV, QA, ARCHITECT, etc)
            available_tools: Tool descriptions from agent.get_available_tools()
            constraints: Task constraints

        Returns:
            Dictionary with:
            - steps: list[dict] - Execution steps
            - reasoning: str - Why this plan
        """
        constraints = constraints or {}

        # Load prompt templates from YAML
        prompt_config = self.prompt_loader.load_prompt_config("plan_generation")

        # Get role-specific prompt
        roles = prompt_config.get("roles", {})
        role_prompt = roles.get(role, f"You are an expert {role} engineer.")

        # Build system prompt from template
        system_template = self.prompt_loader.get_system_prompt_template("plan_generation")
        tools_json = json.dumps(available_tools.capabilities, indent=2)

        system_prompt = system_template.format(
            role_prompt=role_prompt,
            capabilities=tools_json,
            mode=available_tools.mode
        )

        # Build user prompt from template
        user_template = self.prompt_loader.get_user_prompt_template("plan_generation")
        user_prompt = user_template.format(
            task=task,
            context=context
        )

        # Call LLM via port (low-level API call)
        response = await self.llm_client.generate(system_prompt, user_prompt)

        # Parse JSON from response using parser service
        plan = self.json_parser.parse_json_response(response)

        if "steps" not in plan:
            raise ValueError("Plan missing 'steps' field")

        # Convert dict steps to ExecutionStep entities
        step_entities = self.step_mapper.to_entity_list(plan["steps"])

        return PlanDTO(
            steps=step_entities,
            reasoning=plan.get("reasoning", "Generated by LLM"),
        )

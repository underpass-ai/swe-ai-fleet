name: "developer"
model: "deepseek-coder:33b"  # vLLM model name
context_window: 32768
temperature: 0.7
max_tokens: 4096
# Alternative models for different backends:
# ollama_model: "deepseek-coder:33b"
# llama_model: "deepseek-coder-33b-gguf"

version: '3.8'

services:
  # ============================================================================
  # Ray Head (mini cluster para E2E)
  # ============================================================================
  ray-head:
    image: docker.io/rayproject/ray:2.9.0-py310
    container_name: orchestrator-e2e-ray-head
    command: >
      ray start --head --port=6379 --dashboard-host=0.0.0.0 --dashboard-port=8265 --block
    ports:
      - "26379:6379"    # Ray GCS
      - "28265:8265"    # Ray Dashboard
      - "10001:10001"   # Ray Client
    environment:
      - RAY_memory_monitor_refresh_ms=0
    healthcheck:
      test: ["CMD", "ray", "status"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - orchestrator-e2e-network

  # ============================================================================
  # Ray Worker (para ejecutar jobs)
  # ============================================================================
  ray-worker:
    image: docker.io/rayproject/ray:2.9.0-py310
    container_name: orchestrator-e2e-ray-worker
    command: >
      ray start --address=ray-head:6379 --block
    depends_on:
      ray-head:
        condition: service_healthy
    environment:
      - RAY_memory_monitor_refresh_ms=0
    networks:
      - orchestrator-e2e-network

  # ============================================================================
  # vLLM Server (LLM inference)
  # ============================================================================
  vllm:
    image: docker.io/vllm/vllm-openai:latest
    container_name: orchestrator-e2e-vllm
    command: >
      --model TinyLlama/TinyLlama-1.1B-Chat-v1.0
      --host 0.0.0.0
      --port 8000
      --dtype auto
      --max-model-len 2048
    ports:
      - "28000:8000"
    environment:
      - HF_TOKEN=${HF_TOKEN:-}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    # Note: No GPU in E2E containers (uses CPU, slower but works)
    networks:
      - orchestrator-e2e-network

  # ============================================================================
  # NATS with JetStream
  # ============================================================================
  nats:
    image: docker.io/library/nats:2.10-alpine
    container_name: orchestrator-e2e-nats
    command: --jetstream --store_dir=/data
    ports:
      - "24222:4222"
      - "28222:8222"
    healthcheck:
      test: ["CMD", "nats", "server", "check", "jetstream"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - orchestrator-e2e-network

  # ============================================================================
  # Redis (for Context Service dependencies)
  # ============================================================================
  redis:
    image: docker.io/library/redis:7-alpine
    container_name: orchestrator-e2e-redis
    command: redis-server --appendonly yes
    ports:
      - "26379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - orchestrator-e2e-network

  # ============================================================================
  # Orchestrator Service (with Ray + vLLM)
  # ============================================================================
  orchestrator:
    build:
      context: ../../../..
      dockerfile: services/orchestrator/Dockerfile
    container_name: orchestrator-e2e-service
    ports:
      - "50055:50055"
    environment:
      - GRPC_PORT=50055
      - NATS_URL=nats://nats:4222
      - ENABLE_NATS=true
      - RAY_ADDRESS=ray://ray-head:10001
      - VLLM_URL=http://vllm:8000
      - VLLM_MODEL=TinyLlama/TinyLlama-1.1B-Chat-v1.0
      - AGENT_TYPE=vllm
      - PYTHONUNBUFFERED=1
    depends_on:
      nats:
        condition: service_healthy
      redis:
        condition: service_healthy
      ray-head:
        condition: service_healthy
      vllm:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import grpc; channel = grpc.insecure_channel('localhost:50055'); channel.close()"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - orchestrator-e2e-network

  # ============================================================================
  # E2E Tests
  # ============================================================================
  tests:
    build:
      context: ../../../..
      dockerfile: tests/e2e/services/orchestrator/Dockerfile.test
    container_name: orchestrator-e2e-tests
    environment:
      - ORCHESTRATOR_URL=orchestrator:50055
      - NATS_URL=nats://nats:4222
      - REDIS_URL=redis://redis:6379
      - PYTHONPATH=/workspace
    depends_on:
      orchestrator:
        condition: service_healthy
    networks:
      - orchestrator-e2e-network
    command: >
      pytest -m e2e 
      tests/e2e/services/orchestrator/ 
      -v --tb=short
      --ignore=tests/e2e/services/orchestrator/test_ray_vllm_async_e2e.py

networks:
  orchestrator-e2e-network:
    driver: bridge


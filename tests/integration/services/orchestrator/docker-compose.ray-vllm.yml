version: '3.8'

services:
  # ============================================================================
  # Ray Server - Simple Ray instance to execute VLLMAgentJob actors
  # ============================================================================
  ray-server:
    image: docker.io/rayproject/ray:2.50.0-py311
    container_name: orchestrator-e2e-ray
    shm_size: '2gb'  # Required for Ray object store
    command: >
      bash -c "
        pip install --quiet aiohttp nats-py &&
        ray start --head --port=6379 --num-cpus=4 --disable-usage-stats --block
      "
    ports:
      - "36379:6379"    # Ray GCS
      - "30001:10001"   # Ray Client
    environment:
      - RAY_memory_monitor_refresh_ms=0
      - RAY_object_store_memory=1000000000  # 1GB
    healthcheck:
      test: ["CMD", "ray", "status"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s
    networks:
      - orchestrator-e2e-network

  # ============================================================================
  # vLLM Server - LLM inference (CPU mode for E2E)
  # ============================================================================
  vllm:
    image: docker.io/vllm/vllm-openai:latest
    container_name: orchestrator-e2e-vllm
    command: >
      --model TinyLlama/TinyLlama-1.1B-Chat-v1.0
      --host 0.0.0.0
      --port 8000
      --dtype auto
      --max-model-len 2048
      --gpu-memory-utilization 0.5
      --enforce-eager
    ports:
      - "28000:8000"
    devices:
      - nvidia.com/gpu=all
    environment:
      - HF_TOKEN=${HF_TOKEN:-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}
      - CUDA_VISIBLE_DEVICES=0
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 120s  # vLLM takes time to start
    networks:
      - orchestrator-e2e-network

  # ============================================================================
  # NATS with JetStream
  # ============================================================================
  nats:
    image: docker.io/library/nats:2.10-alpine
    container_name: orchestrator-e2e-nats
    command: --jetstream --store_dir=/data
    ports:
      - "24222:4222"
      - "28222:8222"
    healthcheck:
      test: ["CMD", "nats", "server", "check", "jetstream"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - orchestrator-e2e-network

  # ============================================================================
  # Redis (for orchestrator dependencies)
  # ============================================================================
  redis:
    image: docker.io/library/redis:7-alpine
    container_name: orchestrator-e2e-redis
    command: redis-server --appendonly yes
    ports:
      - "26379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - orchestrator-e2e-network

  # ============================================================================
  # Orchestrator Service (with Ray + vLLM integration)
  # ============================================================================
  orchestrator:
    build:
      context: ../../../..
      dockerfile: services/orchestrator/Dockerfile
    container_name: orchestrator-e2e-service
    ports:
      - "50055:50055"
    environment:
      - GRPC_PORT=50055
      - NATS_URL=nats://nats:4222
      - ENABLE_NATS=true
      - RAY_ADDRESS=ray://ray-server:10001
      - VLLM_URL=http://vllm:8000
      - VLLM_MODEL=TinyLlama/TinyLlama-1.1B-Chat-v1.0
      - AGENT_TYPE=vllm
      - DELIBERATION_TIMEOUT=300
      - DELIBERATION_CLEANUP=3600
      - PYTHONUNBUFFERED=1
    depends_on:
      nats:
        condition: service_healthy
      redis:
        condition: service_healthy
      ray-server:
        condition: service_healthy
      vllm:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import grpc; channel = grpc.insecure_channel('localhost:50055'); channel.close()"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 60s
    networks:
      - orchestrator-e2e-network

  # ============================================================================
  # E2E Tests
  # ============================================================================
  tests:
    build:
      context: ../../../..
      dockerfile: tests/integration/services/orchestrator/Dockerfile.test
    container_name: orchestrator-e2e-tests
    environment:
      - ORCHESTRATOR_URL=orchestrator:50055
      - NATS_URL=nats://nats:4222
      - REDIS_URL=redis://redis:6379
      - RAY_ADDRESS=ray://ray-head:10001
      - VLLM_URL=http://vllm:8000
      - PYTHONPATH=/workspace
    depends_on:
      orchestrator:
        condition: service_healthy
    networks:
      - orchestrator-e2e-network
    command: >
      pytest -m e2e 
      tests/integration/services/orchestrator/test_deliberate_e2e.py
      tests/integration/services/orchestrator/test_orchestrate_e2e.py
      -v --tb=short -x

networks:
  orchestrator-e2e-network:
    driver: bridge


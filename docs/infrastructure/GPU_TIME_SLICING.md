# NVIDIA GPU Time-Slicing Configuration

## ğŸ¯ ConfiguraciÃ³n Actual

Tu cluster estÃ¡ usando **NVIDIA Time-Slicing** para multiplicar el nÃºmero de GPUs disponibles:

```
GPUs FÃ­sicas:    4x RTX 3090 (24GB VRAM c/u)
Time-Slicing:    2 replicas por GPU
GPUs Virtuales:  8 (disponibles para Kubernetes)
```

### Estado Actual

```bash
$ kubectl describe node wrx80-node1 | grep nvidia.com/gpu
  nvidia.com/gpu:     8  âœ…
```

---

## ğŸ”§ CÃ³mo Funciona Time-Slicing

**Time-Slicing** permite que **mÃºltiples pods compartan una sola GPU fÃ­sica** mediante time-sharing del contexto CUDA:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GPU FÃ­sica 0 (RTX 3090 - 24GB VRAM)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Slice 0: Pod A (12GB asignado virtual) â”‚
â”‚  Slice 1: Pod B (12GB asignado virtual) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       Time-sharing de contextos CUDA
```

### Ventajas
âœ… **MÃ¡s GPUs disponibles** para schedulear pods  
âœ… **Mejor utilizaciÃ³n** de recursos GPU  
âœ… **Ideal para cargas intermitentes** (LLM inference)

### Desventajas
âš ï¸ **VRAM compartida:** Los 24GB son compartidos entre ambos pods  
âš ï¸ **Latencia variable:** Si ambos pods usan GPU simultÃ¡neamente, hay context switching  
âš ï¸ **No es aislamiento real:** No hay garantÃ­as de VRAM exclusiva

---

## ğŸ“Š ConfiguraciÃ³n del NVIDIA GPU Operator

### ConfigMap Actual

```yaml
# kubectl get cm time-slicing-config -n nvidia -o yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: time-slicing-config
  namespace: nvidia
data:
  default: |
    version: v1
    sharing:
      timeSlicing:
        resources:
          - name: nvidia.com/gpu
            replicas: 2  # â† 2 pods por GPU fÃ­sica
```

### ClusterPolicy

```yaml
# kubectl get clusterpolicy cluster-policy -n nvidia -o yaml
spec:
  devicePlugin:
    config:
      default: default
      name: time-slicing-config  # â† Referencia al ConfigMap
    enabled: true
```

---

## ğŸš€ Uso en RayCluster

Tu RayCluster actual (`ray-gpu` en namespace `ray`) ya estÃ¡ aprovechando las 8 GPUs virtuales:

```yaml
workerGroupSpecs:
  - groupName: gpu-workers
    replicas: 8  # â† 8 workers, 1 GPU virtual c/u
    rayStartParams:
      num-gpus: '1'
    template:
      spec:
        containers:
          - name: ray-worker
            resources:
              requests:
                nvidia.com/gpu: 1  # â† Solicita 1 GPU virtual
              limits:
                nvidia.com/gpu: 1
```

**Resultado:**
- 8 Ray workers corriendo simultÃ¡neamente
- Cada worker cree tener 1 GPU dedicada
- En realidad, estÃ¡n compartiendo las 4 GPUs fÃ­sicas (2 workers por GPU)

---

## ğŸ“ˆ Ajustar Time-Slicing

### Aumentar a 4 pods por GPU (16 GPUs virtuales)

Si quieres mÃ¡s paralelismo (ideal para inference de modelos pequeÃ±os):

```bash
kubectl patch configmap time-slicing-config -n nvidia --type merge -p '
{
  "data": {
    "default": "version: v1\nsharing:\n  timeSlicing:\n    resources:\n      - name: nvidia.com/gpu\n        replicas: 4"
  }
}'

# Reiniciar device plugin para aplicar cambios
kubectl delete pod -n nvidia -l app=nvidia-device-plugin-daemonset
```

Verificar:
```bash
kubectl describe node wrx80-node1 | grep nvidia.com/gpu
# DeberÃ­a mostrar: nvidia.com/gpu: 16
```

### Reducir a 1 pod por GPU (sin time-slicing)

Si necesitas VRAM exclusiva por pod:

```bash
kubectl patch configmap time-slicing-config -n nvidia --type merge -p '
{
  "data": {
    "default": "version: v1\nsharing:\n  timeSlicing:\n    resources:\n      - name: nvidia.com/gpu\n        replicas: 1"
  }
}'

kubectl delete pod -n nvidia -l app=nvidia-device-plugin-daemonset
```

---

## ğŸ§ª Verificar Time-Slicing en AcciÃ³n

### Ver quÃ© pods estÃ¡n usando GPUs

```bash
kubectl get pods -A -o custom-columns=\
NAMESPACE:.metadata.namespace,\
NAME:.metadata.name,\
GPU_REQUEST:.spec.containers[*].resources.requests.'nvidia\.com/gpu',\
NODE:.spec.nodeName | grep -v '<none>'
```

### Ver VRAM usada en cada GPU fÃ­sica

```bash
kubectl exec -it -n ray ray-gpu-gpu-workers-worker-29sp7 -- nvidia-smi

# Output:
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07    Driver Version: 550.90.07    CUDA Version: 12.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
|   0  NVIDIA GeForce ... On   | 00000000:01:00.0 Off |                  N/A |
|-------------------------------+----------------------+----------------------+
|  0%   45C    P8    25W / 350W |  4512MiB / 24576MiB  |      0%      Default |
+-------------------------------+----------------------+----------------------+

# â† 4.5GB usados de 24GB (este pod comparte la GPU con otro pod)
```

### Test de Aislamiento

Ejecutar 2 pods en la misma GPU y ver VRAM:

```bash
# Pod 1
kubectl run gpu-test-1 --image=docker.io/nvidia/cuda:12.4.0-base-ubuntu22.04 \
  --restart=Never --rm -it -n default \
  --overrides='{"spec":{"nodeSelector":{"kubernetes.io/hostname":"wrx80-node1"},"containers":[{"name":"gpu-test","image":"docker.io/nvidia/cuda:12.4.0-base-ubuntu22.04","command":["sh","-c","nvidia-smi && sleep 300"],"resources":{"limits":{"nvidia.com/gpu":"1"}}}]}}'

# En otra terminal - Pod 2
kubectl run gpu-test-2 --image=docker.io/nvidia/cuda:12.4.0-base-ubuntu22.04 \
  --restart=Never --rm -it -n default \
  --overrides='{"spec":{"nodeSelector":{"kubernetes.io/hostname":"wrx80-node1"},"containers":[{"name":"gpu-test","image":"docker.io/nvidia/cuda:12.4.0-base-ubuntu22.04","command":["sh","-c","nvidia-smi && sleep 300"],"resources":{"limits":{"nvidia.com/gpu":"1"}}}]}}'
```

Ambos pods verÃ¡n **la misma GPU fÃ­sica** con `nvidia-smi`, pero Kubernetes los schedulÃ³ en diferentes "slices".

---

## âš ï¸ Consideraciones para LLMs

### Modelos que Caben en VRAM (con time-slicing)

Con 24GB por GPU fÃ­sica y 2 pods compartiendo:

| Modelo              | VRAM TÃ­pica | Â¿Cabe con 2 pods? |
|---------------------|-------------|-------------------|
| Llama 3.3 70B Q4    | ~40GB       | âŒ No (muy grande)|
| Llama 3.1 8B Q4     | ~5GB        | âœ… SÃ­ (10GB total)|
| Qwen2.5 7B          | ~5GB        | âœ… SÃ­             |
| Mistral 7B          | ~5GB        | âœ… SÃ­             |
| Phi-3 3.8B          | ~3GB        | âœ… SÃ­             |
| Gemma 2B            | ~2GB        | âœ… SÃ­             |

**RecomendaciÃ³n:**  
Si tu workload usa modelos â‰¤ 7B quantizados, **time-slicing con 2 replicas es Ã³ptimo**.

### Si Necesitas Modelos Grandes (70B+)

Tienes 3 opciones:

1. **Desactivar time-slicing** (1 GPU por pod):
   ```bash
   replicas: 1
   ```
   Pero pierdes paralelismo.

2. **Usar tensor parallelism** (multi-GPU por modelo):
   ```bash
   # vLLM con 2 GPUs por modelo
   nvidia.com/gpu: 2
   ```

3. **Pipeline parallelism** (modelo dividido entre GPUs):
   ```python
   # DeepSpeed / Hugging Face Accelerate
   ```

---

## ğŸ“š Referencias

- [NVIDIA Time-Slicing Guide](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-sharing.html)
- [KubeRay + GPU Sharing](https://docs.ray.io/en/latest/cluster/kubernetes/user-guides/gpu.html)
- [GPU Operator ClusterPolicy](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/getting-started.html#cluster-policy-options)

---

## ğŸ¯ PrÃ³ximos Pasos

- âœ… Time-slicing configurado (2 replicas)
- âœ… RayCluster usando 8 GPUs virtuales
- â¬œ Monitorear uso real de VRAM
- â¬œ Ajustar replicas segÃºn carga (2/4/8)
- â¬œ Implementar mÃ©tricas de GPU en Prometheus

# ğŸ§ª Testing Architecture - SWE AI Fleet

**Documento Normativo y ArquitectÃ³nico**

Este documento define la **estrategia de testing completa** para SWE AI Fleet, alineada con los principios de arquitectura hexagonal, Domain-Driven Design, y clean architecture que rigen el proyecto.

---

## ğŸ“ Principios ArquitectÃ³nicos de Testing

### 1. Testing Pyramid

```
                    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
                    â•‘   E2E Tests       â•‘  ~10% (12 tests)
                    â•‘   Full System     â•‘  Cluster K8s
                    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  ~3-5 min
                  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
                  â•‘  Integration Tests      â•‘  ~20% (45 tests)
                  â•‘  Component Boundaries   â•‘  Containers
                  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  ~30-60s
            â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
            â•‘         Unit Tests                  â•‘  ~70% (596 tests)
            â•‘  Domain Logic + Adapters            â•‘  Mocks
            â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  ~3s
```

**Ratio Ideal**: 70% unit / 20% integration / 10% E2E  
**Coverage MÃ­nimo**: 90% en nuevo cÃ³digo (SonarQube quality gate)

### 2. Separation of Concerns en Tests

```
tests/
â”œâ”€â”€ unit/                    â† Domain + Application + Adapters (isolated)
â”‚   â”œâ”€â”€ orchestrator/        â† Pure business logic
â”‚   â”œâ”€â”€ context/            â† Domain entities & use cases
â”‚   â”œâ”€â”€ agents/             â† Agent implementations
â”‚   â””â”€â”€ tools/              â† Tool implementations
â”‚
â”œâ”€â”€ integration/             â† Port implementations (real infra)
â”‚   â”œâ”€â”€ services/orchestrator/  â† gRPC + NATS + Ray
â”‚   â”œâ”€â”€ services/context/       â† gRPC + Neo4j + Valkey
â”‚   â””â”€â”€ archived/              â† Legacy tests (reference)
â”‚
â””â”€â”€ e2e/                     â† Full system (deployed K8s)
    â”œâ”€â”€ test_system_e2e.py      â† Complete workflows
    â”œâ”€â”€ test_orchestrator_cluster.py  â† Deliberation E2E
    â””â”€â”€ test_ray_vllm_e2e.py    â† GPU execution E2E
```

**Principio**: Tests siguen la estructura hexagonal del cÃ³digo.

### 3. Test Isolation & Independence

#### Unit Tests - ISOLATED
- âœ… NO external dependencies (DB, API, containers)
- âœ… Mocks para ports (MessagingPort, GraphQueryPort, etc.)
- âœ… Fast (<5s total)
- âœ… DeterminÃ­sticos (mismo input â†’ mismo output)

#### Integration Tests - BOUNDED
- âœ… Real infrastructure (Redis, Neo4j, NATS)
- âœ… Test containers (ephemeral)
- âœ… Cleanup automÃ¡tico
- âœ… Aislados entre sÃ­ (cada test tiene su propio estado)

#### E2E Tests - COMPLETE
- âœ… Full deployed system (K8s)
- âœ… Real services communicating
- âœ… GPU execution validated
- âœ… Production-like environment

---

## ğŸ¯ Punto de Entrada Ãšnico: Makefile

### FilosofÃ­a

**NO hay pytest directo en este proyecto.**  
**TODO pasa por el Makefile.**

**RazÃ³n**: Tests requieren setup complejo:
1. GeneraciÃ³n de protobuf stubs
2. Levantamiento de containers
3. ActivaciÃ³n de `.venv`
4. Cleanup de artifacts

El Makefile **garantiza** que todo el setup se ejecuta correctamente.

### Targets Disponibles

```makefile
make test              # Alias para test-unit (default)
make test-unit         # Unit tests (~3s)
make test-integration  # Integration tests (~45s)
make test-e2e          # E2E tests (~3-5min)
make test-all          # All tests (~4-5min)
make test-coverage     # Coverage report (CI)
```

### Arquitectura del Makefile

```
Makefile (root)
    â”‚
    â”œâ”€â†’ test-unit         â†’ scripts/test/unit.sh
    â”‚   â”œâ”€ Activate .venv
    â”‚   â”œâ”€ Generate protobuf stubs (/tmp)
    â”‚   â”œâ”€ pytest -m 'not e2e and not integration'
    â”‚   â””â”€ Cleanup stubs
    â”‚
    â”œâ”€â†’ test-integration  â†’ scripts/test/integration.sh
    â”‚   â”œâ”€ Start containers (podman-compose)
    â”‚   â”œâ”€ Wait for services ready
    â”‚   â”œâ”€ pytest -m integration
    â”‚   â””â”€ Stop & cleanup containers
    â”‚
    â”œâ”€â†’ test-e2e          â†’ scripts/test/e2e.sh
    â”‚   â”œâ”€ Verify K8s cluster accessible
    â”‚   â”œâ”€ Check services deployed
    â”‚   â”œâ”€ pytest -m e2e
    â”‚   â””â”€ Generate report
    â”‚
    â”œâ”€â†’ test-coverage     â†’ scripts/test/coverage.sh
    â”‚   â”œâ”€ pytest --cov=swe_ai_fleet --cov-report=xml
    â”‚   â””â”€ Generate coverage.xml (SonarQube)
    â”‚
    â””â”€â†’ test-all          â†’ scripts/test/all.sh
        â”œâ”€ make test-unit
        â”œâ”€ make test-integration
        â”œâ”€ make test-e2e
        â””â”€ Consolidate results
```

---

## ğŸ§© Testing Strategy por Capa

### Domain Layer Tests (Unit)

**Objetivo**: Validar lÃ³gica de negocio pura

**CaracterÃ­sticas**:
- NO dependencies externas
- Pure domain entities (Agent, Task, Council, etc.)
- Business rules validation
- Value objects immutability

**Ejemplo**:
```python
# tests/unit/orchestrator/domain/agents/test_mock_agent.py

def test_mock_agent_excellent_behavior():
    """Test que MockAgent con EXCELLENT behavior genera high-quality proposals."""
    agent = MockAgent(
        agent_id="test-001",
        role="DEV",
        behavior=AgentBehavior.EXCELLENT,
        seed=42
    )
    
    result = agent.generate("Implement login", constraints, diversity=False)
    
    assert "high quality" in result["content"].lower()
    assert len(result["content"]) > 100
```

**Principio**: Domain tests NO conocen infrastructure (gRPC, NATS, HTTP, etc.)

---

### Application Layer Tests (Unit)

**Objetivo**: Validar use cases y orchestration logic

**CaracterÃ­sticas**:
- Mocks de ports (interfaces)
- Orchestration flow validation
- Error handling y edge cases
- Event publishing verification

**Ejemplo**:
```python
# services/orchestrator/tests/application/test_deliberate_usecase.py

@pytest.mark.asyncio
async def test_deliberate_publishes_event():
    """Test que DeliberateUseCase publica DeliberationCompletedEvent."""
    mock_messaging = AsyncMock(spec=MessagingPort)
    stats = OrchestratorStatistics()
    
    use_case = DeliberateUseCase(stats=stats, messaging=mock_messaging)
    
    await use_case.execute(
        council=mock_council,
        role="DEV",
        task_description="Test task",
        constraints=mock_constraints,
        story_id="story-1",
        task_id="task-1"
    )
    
    # Verify event published
    mock_messaging.publish.assert_called_once()
    call_args = mock_messaging.publish.call_args
    assert call_args[0][0] == "orchestration.deliberation.completed"
    assert isinstance(call_args[0][1], DeliberationCompletedEvent)
```

**Principio**: Use cases orchestran domain + ports, NO infrastructure.

---

### Infrastructure Layer Tests (Integration)

**Objetivo**: Validar adapters con infraestructura real

**CaracterÃ­sticas**:
- Real external systems (Redis, Neo4j, NATS, vLLM)
- Container-based (testcontainers pattern)
- Port contract verification
- Network/timeout/retry testing

**Ejemplo**:
```python
# tests/integration/services/orchestrator/test_grpc_integration.py

@pytest.mark.integration
def test_orchestrator_grpc_create_council():
    """Test gRPC CreateCouncil con servidor real."""
    # Setup: Real gRPC server en container
    channel = grpc.insecure_channel('localhost:50055')
    stub = OrchestratorServiceStub(channel)
    
    # Act: Call real gRPC endpoint
    request = CreateCouncilRequest(role="DEV", num_agents=3)
    response = stub.CreateCouncil(request)
    
    # Assert: Verify response contract
    assert response.council_id
    assert len(response.agent_ids) == 3
```

**Principio**: Integration tests validan **contratos de ports**, no lÃ³gica de negocio.

---

### End-to-End Tests (E2E)

**Objetivo**: Validar sistema completo deployed

**CaracterÃ­sticas**:
- Full Kubernetes deployment
- Real microservices communication
- GPU execution verification
- Production-like environment

**Ejemplo**:
```python
# tests/e2e/test_orchestrator_cluster.py

@pytest.mark.e2e
def test_full_deliberation_workflow():
    """Test workflow completo: Planning â†’ Orchestrator â†’ Ray â†’ GPU."""
    # 1. Publish plan.approved event to NATS
    publish_plan_approved(story_id="story-e2e", plan_id="plan-e2e", roles=["DEV"])
    
    # 2. Wait for auto-dispatch
    time.sleep(65)
    
    # 3. Verify deliberation completed
    logs = kubectl_logs("orchestrator", since="70s")
    assert "âœ… Deliberation completed for DEV: 3 proposals in" in logs
    assert "50000ms" < duration < "80000ms"  # Real vLLM timing
    
    # 4. Verify GPU was used
    vllm_logs = kubectl_logs("vllm-server", since="70s")
    assert "POST /v1/chat/completions" in vllm_logs
```

**Principio**: E2E tests validan **comportamiento observable** del sistema deployed.

---

## ğŸ”§ Convenciones de Testing

### Naming Conventions

```python
# Unit tests
def test_<what>_<condition>_<expected>():
    """Test that <component> <does what> when <condition>."""
    pass

# Integration tests
@pytest.mark.integration
def test_<adapter>_<operation>_<with_real_infra>():
    """Test <adapter> <operation> with real <infrastructure>."""
    pass

# E2E tests
@pytest.mark.e2e
def test_<workflow>_<scenario>_e2e():
    """Test <workflow> end-to-end in <scenario>."""
    pass
```

**Ejemplos**:
- `test_deliberate_empty_role_raises()` - Unit
- `test_nats_adapter_publish_with_real_nats()` - Integration
- `test_full_deliberation_workflow_e2e()` - E2E

### Fixtures Organization

```python
# tests/conftest.py - Root fixtures (shared)
@pytest.fixture
def mock_agent():
    """Create mock agent for unit tests."""
    return MockAgent(agent_id="test-001", role="DEV")

# tests/unit/conftest.py - Unit-specific fixtures
@pytest.fixture
def mock_council_registry():
    """Mock CouncilRegistry for use case tests."""
    return MagicMock(spec=CouncilRegistry)

# tests/integration/conftest.py - Integration fixtures
@pytest.fixture(scope="session")
def nats_container():
    """Start real NATS container for integration tests."""
    # testcontainers pattern
    pass
```

**Principio**: Fixtures en el nivel mÃ¡s bajo posible (DRY + proximity).

### Async Testing

```python
# âœ… Correct
@pytest.mark.asyncio
async def test_deliberate_async():
    result = await use_case.execute(...)
    assert result.duration_ms > 0

# âŒ Incorrect
def test_deliberate_sync():
    result = use_case.execute(...)  # Coroutine not awaited!
```

**Regla**: Si el cÃ³digo es async, el test DEBE ser async.

---

## ğŸš¨ Anti-Patterns (AVOID)

### âŒ Running pytest Directly

```bash
# âŒ NUNCA hacer esto
pytest tests/unit/                    # Falta protobuf setup
python -m pytest tests/               # Falta .venv
pytest -m integration                 # Falta containers

# âœ… SIEMPRE hacer esto
make test-unit                        # âœ… Setup completo
make test-integration                 # âœ… Containers + cleanup
make test-e2e                        # âœ… Cluster verification
```

### âŒ Tests con Side Effects

```python
# âŒ Test modifica estado global
def test_bad():
    global_registry.clear()  # âŒ Afecta otros tests
    ...

# âœ… Test aislado con fixture
def test_good(registry):
    registry.clear()  # âœ… Registry local del fixture
    ...
```

### âŒ Tests que Dependen de Orden

```python
# âŒ test_2 depende de que test_1 corriÃ³ antes
def test_1_create():
    db.insert(...)

def test_2_read():
    assert db.get(...)  # âŒ Depende de test_1

# âœ… Cada test independiente
@pytest.fixture
def setup_data(db):
    db.insert(...)
    yield
    db.clear()

def test_2_read(setup_data):
    assert db.get(...)  # âœ… Setup explÃ­cito
```

### âŒ Tests que Asumen Estructura de API

```python
# âŒ Asumir campos sin verificar .proto
def test_response():
    assert response.success  # âŒ Campo no existe en proto!

# âœ… Verificar proto PRIMERO, luego escribir test
def test_response():
    # Proto tiene: council_id, agent_ids (no "success")
    assert response.council_id
    assert len(response.agent_ids) == 3
```

**Memoria**: [[memory:9776848]] - SIEMPRE verificar .proto antes de escribir tests.

---

## ğŸ“Š Test Execution Architecture

### Entry Point: Makefile

```mermaid
graph TD
    A[Developer] -->|make test-unit| B[Makefile]
    B --> C[scripts/test/unit.sh]
    C --> D[Activate .venv]
    D --> E[Generate protobuf /tmp]
    E --> F[pytest -m not e2e and not integration]
    F --> G[Cleanup stubs]
    G --> H[Report results]
    
    A -->|make test-integration| I[scripts/test/integration.sh]
    I --> J[podman-compose up]
    J --> K[Wait for services ready]
    K --> L[pytest -m integration]
    L --> M[podman-compose down]
    M --> H
    
    A -->|make test-e2e| N[scripts/test/e2e.sh]
    N --> O[Verify K8s cluster]
    O --> P[Check services deployed]
    P --> Q[pytest -m e2e]
    Q --> H
```

### Test Lifecycle

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. SETUP PHASE                                 â”‚
â”‚  - Activate .venv                               â”‚
â”‚  - Generate protobuf (if needed)                â”‚
â”‚  - Start containers (if integration)            â”‚
â”‚  - Verify cluster (if E2E)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. EXECUTION PHASE                             â”‚
â”‚  - pytest discovers tests                       â”‚
â”‚  - Runs tests in parallel (xdist if enabled)    â”‚
â”‚  - Collects results                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. REPORTING PHASE                             â”‚
â”‚  - Generate coverage.xml (if --cov)             â”‚
â”‚  - Create HTML report (if requested)            â”‚
â”‚  - Print summary                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. CLEANUP PHASE                               â”‚
â”‚  - Remove generated stubs                       â”‚
â”‚  - Stop containers (if integration)             â”‚
â”‚  - Clear temp files                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—ï¸ Testing Hexagonal Architecture

### Ports Testing Strategy

```python
# Port (interface)
class MessagingPort(ABC):
    @abstractmethod
    async def publish(self, subject: str, event: DomainEvent) -> None:
        pass

# Unit test: Mock the port
def test_use_case_with_mock_messaging():
    mock_messaging = AsyncMock(spec=MessagingPort)
    use_case = SomeUseCase(messaging=mock_messaging)
    
    await use_case.execute(...)
    
    mock_messaging.publish.assert_called_once()

# Integration test: Real adapter
@pytest.mark.integration
def test_nats_adapter_implements_messaging_port():
    nats_adapter = NatsMessagingAdapter(url="nats://localhost:4222")
    
    # Verify port contract
    assert isinstance(nats_adapter, MessagingPort)
    
    # Test real operation
    await nats_adapter.publish("test.subject", test_event)
    
    # Verify event received
    msg = await nats_subscriber.fetch(1, timeout=2.0)
    assert msg.data == test_event.to_dict()
```

**Principio**: 
- **Unit tests**: Test use cases con **mock ports**
- **Integration tests**: Test adapters con **real infrastructure**

### Domain Logic Testing

```python
# Domain entity
@dataclass(frozen=True)
class DeliberationResult:
    proposal: Proposal
    checks: CheckSuiteResult
    score: float
    
    def to_dict(self) -> dict[str, Any]:
        """Serialization for events."""
        return {
            "proposal": self.proposal.to_dict(),
            "checks": self.checks.to_dict(),
            "score": self.score,
        }

# Unit test
def test_deliberation_result_to_dict():
    """Test que DeliberationResult serializa correctamente."""
    agent = MockAgent(agent_id="a1", role="DEV")
    proposal = Proposal(author=agent, content="Test")
    checks = CheckSuiteResult(lint=..., dryrun=..., policy=...)
    
    result = DeliberationResult(proposal=proposal, checks=checks, score=0.85)
    
    data = result.to_dict()
    
    assert data["proposal"]["author"]["agent_id"] == "a1"
    assert data["proposal"]["content"] == "Test"
    assert data["score"] == 0.85
    assert "checks" in data
```

**Principio**: Domain entities son **self-contained y testables sin mocks**.

---

## ğŸ¯ Coverage Strategy

### Coverage Goals

| Layer | Target | Actual | Status |
|-------|--------|--------|--------|
| **Domain** | 95% | 96% | âœ… |
| **Application** | 90% | 92% | âœ… |
| **Infrastructure** | 80% | 84% | âœ… |
| **Overall** | 90% | 92% | âœ… |

### Coverage Enforcement

**SonarQube Quality Gate**:
- **90% minimum** en nuevo cÃ³digo
- Solo unit tests cuentan (no e2e/integration)
- Branch coverage incluido

**pytest.ini**:
```ini
[tool:pytest]
testpaths = tests/unit
markers =
    integration: Integration tests (containers)
    e2e: End-to-end tests (K8s cluster)
```

**CI Command**:
```bash
make test-coverage
# Genera coverage.xml para SonarQube
```

### Coverage Exclusions

```python
# Excluir de coverage (solo cuando justificado)
if TYPE_CHECKING:  # pragma: no cover
    from typing import Protocol

def _debug_helper():  # pragma: no cover
    """Helper solo para debugging."""
    pass
```

**Regla**: Exclusions deben ser **justificadas** (type checking, debug, abstract).

---

## ğŸ” Test Quality Metrics

### Assertion Density

**Good Test** (single responsibility):
```python
def test_council_registers_agents():
    """Test que registry almacena agents correctamente."""
    registry = CouncilRegistry()
    agents = [create_agent(f"a{i}") for i in range(3)]
    
    registry.register_council("DEV", mock_council, agents)
    
    assert registry.has_council("DEV")  # âœ… Single assertion
```

**Bad Test** (multiple responsibilities):
```python
def test_everything():
    """Test que hace todo."""
    registry.register_council(...)
    assert registry.has_council("DEV")
    
    council = registry.get_council("DEV")
    assert len(council.agents) == 3
    
    result = council.deliberate(...)
    assert len(result.proposals) > 0
    # âŒ Demasiadas responsabilidades
```

### Test Readability

**AAA Pattern** (Arrange-Act-Assert):
```python
def test_auto_dispatch_triggers_deliberation():
    """Test que AutoDispatchService ejecuta deliberaciÃ³n."""
    # ARRANGE
    mock_council_query = create_mock_council_query()
    mock_messaging = create_mock_messaging()
    service = AutoDispatchService(
        council_query=mock_council_query,
        messaging=mock_messaging
    )
    event = create_plan_approved_event(roles=["DEV"])
    
    # ACT
    result = await service.dispatch_deliberations_for_plan(event)
    
    # ASSERT
    assert result["successful"] == 1
    assert result["total_roles"] == 1
    mock_messaging.publish.assert_called()
```

**Principio**: Tests son **documentaciÃ³n ejecutable** del comportamiento del sistema.

---

## ğŸ¨ Mock Strategy

### Mock Hierarchy

```
MockAgent (Production Quality)
    â”œâ”€ AgentBehavior.EXCELLENT    â† High-quality proposals
    â”œâ”€ AgentBehavior.NORMAL       â† Average quality
    â”œâ”€ AgentBehavior.POOR         â† Low quality (error testing)
    â”œâ”€ AgentBehavior.STUBBORN     â† Never accepts feedback
    â””â”€ AgentBehavior.RANDOM       â† Non-deterministic

VLLMAgent (Real LLM)
    â””â”€ Production use only (integration/e2e tests)
```

**Principio**: Mocks son **first-class citizens**, bien testeados.

### Factory Functions

```python
# tests/conftest.py

def create_mock_council(role: str = "DEV", num_agents: int = 3):
    """Factory para crear council mock."""
    agents = [
        MockAgent(f"agent-{i}", role, AgentBehavior.NORMAL)
        for i in range(num_agents)
    ]
    return Council(agents=agents, tooling=mock_scoring, rounds=1)

# Usage
def test_something():
    council = create_mock_council(role="QA", num_agents=5)
    # Test con council pre-configurado
```

**Memoria**: [[memory:9776848]] - Mocks configurables y realistas.

---

## ğŸ“ Protobuf Generation Strategy

### Why NOT Commit Generated Code

**RazÃ³n**: Generated code cambia frecuentemente y causa merge conflicts.

**Pattern**: Generate during Docker build & test run.

```dockerfile
# Dockerfile
RUN python -m grpc_tools.protoc \
    --proto_path=/app/specs \
    --python_out=/app/gen \
    --grpc_python_out=/app/gen \
    /app/specs/*.proto
```

```bash
# scripts/test/unit.sh
TEMP_GEN_DIR=$(mktemp -d)
python -m grpc_tools.protoc \
    --proto_path=specs \
    --python_out=$TEMP_GEN_DIR \
    specs/*.proto

# Add to PYTHONPATH
export PYTHONPATH="$TEMP_GEN_DIR:$PYTHONPATH"

# Run tests
pytest ...

# Cleanup
rm -rf $TEMP_GEN_DIR
```

**Principio**: Code generation es **build-time concern**, no source control.

---

## ğŸ¯ CI/CD Integration

### GitHub Actions / GitLab CI

```yaml
# .github/workflows/test.yml

jobs:
  unit-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
      - run: make install-deps
      - run: make test-coverage  # â† Ãšnico comando
      - uses: codecov/codecov-action@v3
        with:
          files: ./coverage.xml

  integration-tests:
    runs-on: ubuntu-latest
    services:
      redis:
        image: docker.io/library/redis:7-alpine
      nats:
        image: docker.io/library/nats:2.10-alpine
    steps:
      - uses: actions/checkout@v3
      - run: make test-integration  # â† Ãšnico comando

  e2e-tests:
    runs-on: self-hosted  # K8s cluster
    steps:
      - uses: actions/checkout@v3
      - run: make test-e2e  # â† Ãšnico comando
```

**Principio**: CI ejecuta **mismo comando** que desarrolladores localmente.

---

## ğŸ” Test Secrets & Configuration

### Environment Variables

```bash
# tests/.env.test (gitignored)
VLLM_URL=http://localhost:8000
NATS_URL=nats://localhost:4222
NEO4J_URI=bolt://localhost:7687
REDIS_URL=redis://localhost:6379
```

### Fixtures con Config

```python
# tests/integration/conftest.py

@pytest.fixture(scope="session")
def vllm_url():
    """Get vLLM URL from env or default."""
    return os.getenv("VLLM_URL", "http://localhost:8000")

@pytest.fixture
def vllm_agent(vllm_url):
    """Create real VLLMAgent for integration tests."""
    return VLLMAgent(
        agent_id="integration-test-001",
        role="DEV",
        vllm_url=vllm_url,
        model="Qwen/Qwen3-0.6B"
    )
```

---

## ğŸ“ˆ Test Metrics & Monitoring

### Current Metrics (21 Oct 2025)

```
Total Tests: 653
â”œâ”€ Unit:        596 (91%)  âœ… 100% passing
â”œâ”€ Integration:  45 (7%)   âœ… 100% passing
â””â”€ E2E:          12 (2%)   âœ… 83% passing (2 skipped)

Execution Time:
â”œâ”€ Unit:        ~3s
â”œâ”€ Integration: ~45s
â””â”€ E2E:         ~3-5min

Coverage:
â”œâ”€ Domain:      96%
â”œâ”€ Application: 92%
â”œâ”€ Infrastructure: 84%
â””â”€ Overall:     92% âœ… (target: 90%)
```

### Monitoring Command

```bash
# Watch test execution
make test-unit | tee test-results.log

# Coverage trending
make test-coverage
open htmlcov/index.html

# CI metrics
cat coverage.xml | grep 'line-rate'
```

---

## ğŸš€ Development Workflow

### Pre-Commit Checklist

```bash
# 1. Activar virtual environment
source .venv/bin/activate

# 2. Run unit tests (fast feedback)
make test-unit
# âœ… 596 passed in 2.88s

# 3. Run linter
ruff check . --fix

# 4. Si cambios en adapters, run integration tests
make test-integration
# âœ… 45 passed in 42s

# 5. Commit
git add .
git commit -m "feat: ..."
```

### Pre-PR Checklist

```bash
# 1. All tests
make test-all
# âœ… 651 passed in 4m

# 2. Coverage check
make test-coverage
# âœ… 92% coverage (>90% required)

# 3. E2E en cluster (si cambios en microservices)
make test-e2e
# âœ… 10 passed in 3m

# 4. Push
git push origin feature/my-feature
```

### CI Pipeline

```bash
# GitHub Actions ejecuta:
make test-coverage    # Quality gate (90%)
make test-all        # Full verification

# Si pasa â†’ merge permitido
# Si falla â†’ bloquea PR
```

---

## ğŸ”§ Troubleshooting

### "ModuleNotFoundError: services.*.gen"

**Causa**: Protobuf stubs no generados  
**SoluciÃ³n**: NO uses pytest directo, usa `make test-unit`

**ExplicaciÃ³n**:
```bash
# âŒ pytest directo falla
pytest tests/unit/services/orchestrator/

# âœ… Makefile genera stubs automÃ¡ticamente
make test-unit
```

---

### "Connection refused" en Integration Tests

**Causa**: Containers no levantados  
**SoluciÃ³n**: Usa `make test-integration`

**Debug**:
```bash
# Verificar containers
podman ps | grep -E "redis|nats|neo4j"

# Si no hay containers, el Makefile los levanta automÃ¡ticamente
make test-integration
```

---

### E2E Tests Fallan con "Service not found"

**Causa**: Servicios no desplegados en K8s  
**SoluciÃ³n**: Deploy primero

```bash
# 1. Verificar cluster
kubectl get nodes

# 2. Deploy servicios
./scripts/infra/deploy-all.sh

# 3. Verificar health
./scripts/infra/verify-health.sh

# 4. Run E2E
make test-e2e
```

---

### Tests Pasan Localmente pero Fallan en CI

**Posibles causas**:
1. **Timestamps/UUIDs**: Tests dependen de valores no determinÃ­sticos
2. **Race conditions**: Tests async sin proper waiting
3. **Shared state**: Tests modifican estado global
4. **Missing .venv activation**: CI no activÃ³ virtual environment

**SoluciÃ³n**: 
```python
# âœ… Usar fixtures con seed determinÃ­stico
@pytest.fixture
def fixed_seed():
    random.seed(42)
    yield
    random.seed()  # Reset

# âœ… Usar freezegun para timestamps
@freeze_time("2025-10-21 12:00:00")
def test_with_fixed_time():
    event = create_event()  # timestamp serÃ¡ fijo
```

---

## ğŸ“š Test Organization Best Practices

### Directory Structure Mirrors Code

```
src/swe_ai_fleet/orchestrator/
    â”œâ”€â”€ domain/
    â”‚   â”œâ”€â”€ agents/
    â”‚   â”‚   â””â”€â”€ vllm_agent.py
    â”‚   â””â”€â”€ deliberation_result.py
    â”œâ”€â”€ application/
    â”‚   â””â”€â”€ usecases/
    â”‚       â””â”€â”€ deliberate_usecase.py
    â””â”€â”€ infrastructure/
        â””â”€â”€ adapters/
            â””â”€â”€ nats_messaging_adapter.py

tests/unit/orchestrator/
    â”œâ”€â”€ domain/
    â”‚   â”œâ”€â”€ agents/
    â”‚   â”‚   â””â”€â”€ test_vllm_agent.py      â† Mirrors domain/agents/
    â”‚   â””â”€â”€ test_deliberation_result.py
    â”œâ”€â”€ application/
    â”‚   â””â”€â”€ test_deliberate_usecase.py   â† Mirrors application/usecases/
    â””â”€â”€ infrastructure/
        â””â”€â”€ test_nats_adapter.py          â† Mirrors infrastructure/adapters/
```

**Principio**: Test path mirrors source path (fÃ¡cil navegaciÃ³n).

### Test File Naming

```
# Source file
src/swe_ai_fleet/orchestrator/domain/agents/vllm_agent.py

# Test files
tests/unit/agents/test_vllm_agent_unit.py              # Unit
tests/integration/orchestrator/test_vllm_agent_integration.py  # Integration
tests/e2e/test_vllm_agent_e2e.py                      # E2E

# Suffix indica tipo de test
_unit.py         â†’ Unit test
_integration.py  â†’ Integration test
_e2e.py         â†’ E2E test
```

---

## ğŸ“ Advanced Testing Patterns

### Property-Based Testing

```python
# Para domain entities con invariants
from hypothesis import given, strategies as st

@given(
    agent_id=st.text(min_size=1),
    role=st.sampled_from(["DEV", "QA", "ARCHITECT"]),
    temperature=st.floats(min_value=0.0, max_value=2.0)
)
def test_agent_config_invariants(agent_id, role, temperature):
    """Test que AgentConfig mantiene invariants con cualquier input."""
    config = AgentConfig(
        agent_id=agent_id,
        role=role,
        vllm_url="http://test",
        model="test-model",
        temperature=temperature
    )
    
    # Invariants
    assert config.agent_id == agent_id
    assert 0.0 <= config.temperature <= 2.0
    assert config.to_dict()["agent_id"] == agent_id
```

### Parametrized Tests

```python
@pytest.mark.parametrize("role,expected_agents", [
    ("DEV", 3),
    ("QA", 3),
    ("ARCHITECT", 3),
    ("DEVOPS", 3),
    ("DATA", 3),
])
def test_council_initialization(role, expected_agents):
    """Test que todos los roles inicializan con 3 agents."""
    council = create_default_council(role)
    assert len(council.agents) == expected_agents
```

### Snapshot Testing

```python
def test_event_serialization_snapshot(snapshot):
    """Test que event serialization es stable."""
    event = DeliberationCompletedEvent(
        story_id="s1",
        task_id="t1",
        decisions=[...],
        timestamp=1234567890.0  # Fixed
    )
    
    snapshot.assert_match(json.dumps(event.to_dict(), indent=2))
    # Detecta cambios involuntarios en serialization
```

---

## ğŸ† Quality Gates

### Pre-Commit Gate

```bash
make test-unit && ruff check . --fix
# Si pasa â†’ commit permitido
# Si falla â†’ fix required
```

### Pre-Push Gate

```bash
make test-all
# Si pasa â†’ push permitido
# Si falla â†’ fix required
```

### Pre-Merge Gate (CI)

```bash
make test-coverage  # 90% minimum
make test-all      # All tests pass
sonar-scanner      # Quality gate pass
```

**Regla Estricta**: [[memory:9722904]] - NUNCA commit sin tests passing.

---

## ğŸ“– Referencias

### Internal Docs
- [Testing Strategy](TESTING_STRATEGY.md) - Strategy overview
- [Development Guide](DEVELOPMENT_GUIDE.md) - Dev workflow
- [Git Workflow](GIT_WORKFLOW.md) - Commit/PR process

### External Resources
- [Pytest Documentation](https://docs.pytest.org/)
- [Hexagonal Architecture Testing](https://herbertograca.com/2017/09/21/ports-adapters-architecture/)
- [Test Pyramid](https://martinfowler.com/articles/practical-test-pyramid.html)

### Project-Specific
- [Makefile](../Makefile) - Test targets
- [scripts/test/README.md](../scripts/test/README.md) - Script details
- [pytest.ini](../pytest.ini) - Pytest configuration

---

## ğŸ¯ Summary

### âœ… DO
- Use `make test-*` for ALL test execution
- Write tests following hexagonal boundaries
- Mock ports, test adapters with real infra
- Maintain 90%+ coverage on new code
- Follow AAA pattern (Arrange-Act-Assert)

### âŒ DON'T
- Run `pytest` directly (use Makefile)
- Commit generated protobuf code
- Write tests with side effects
- Skip tests before commit
- Mix test types (unit with integration logic)

---

**Ãšltima ActualizaciÃ³n**: 21 de Octubre de 2025  
**Mantenedor**: Tirso (Project Owner & Lead Architect)  
**Status**: âœ… NORMATIVO - Documento autoritativo Ãºnico

---

ğŸ¯ **TL;DR**: `make test-unit` para desarrollo diario, `make test-all` antes de PR.  
TODO lo demÃ¡s es manejado automÃ¡ticamente por el Makefile.


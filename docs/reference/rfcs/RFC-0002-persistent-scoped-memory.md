# RFC-0002: Persistent, Scoped Memory for Multi-Agent SWE

## Problem

Current interactions with LLMs lack durable, structured memory suitable for multi-agent software engineering workflows. Specifically:

- Context windows are limited; long conversations overflow and lose coherence without summarization.
- Knowledge from one use case can bleed into others; we need per–use-case isolation and session scoping.
- There is no persistent store to resume work later or query prior decisions and rationale.
- Decisions are not explicitly logged, making auditability and traceability (who said what, when, and why) difficult.
- Message histories are unstructured and not indexed by role, stage, or task, which impedes targeted retrieval.
- The platform lacks orchestration-aware memory that supports iterative loops across design, implementation, testing, and documentation.

We need a memory architecture that provides persistent, searchable, and scoped context for each use case, supports periodic summarization, explicit decision logging, role/stage metadata, and integrates with multi-agent orchestration to enable continuity, auditability, and reuse of knowledge.

## Solution

##### Intelligent design of persistence and context in the simulation of a development team

###### Atomic use cases and per–use-case context

An atomic use case involves decomposing functionality into manageable, independent units. Each use case should have its own isolated context to prevent information contamination between scenarios. In practice, this means the conversation or process for each use case is handled in a separate memory session. Agent frameworks recommend limiting the scope of context to each flow or task, so agents access only the information that is relevant to that task. This “one context per use case” approach ensures clarity and focus: agents (or participants) consult only data pertinent to the functionality at hand, reducing the risk of mixing requirements or solutions from different cases.

To facilitate this, the human Product Owner (PO) can be provided with a user interface (UI) where they describe each use case independently. When creating a use case in the UI, the system initializes a new, empty context specific to that case. From there, the corresponding virtual agents (analysts, developers, testers, etc.) take that definition and discuss it strictly within that scope. Additionally, each phase is subdivided into atomic subtasks that two agents resolve through dialogue focused on that sub-problem, with the context limited to what is necessary for that subtask.

Defining atomic use cases with isolated contexts follows the same principle of modularity: each use case has its own “memory space” where conversation and decisions related to that requirement are stored, without dragging along histories from other cases.

This isolation provides several benefits. On one hand, it prevents decisions or details from one use case from unduly influencing another, preserving the integrity of discussions. On the other hand, it facilitates agent refocusing: when starting a new case, agents begin with a clean context containing only the information provided by the PO for that case.

In practice, we can implement separate persistence using session identifiers or per-context storage (e.g., namespaces in a vector database or separate collections) for each use case, so that agent memory queries retrieve knowledge only from the current use case.

Finally, a UI for PO-created use cases not only initializes the context, but could also load prior contexts if a use case is resumed later. Since AI memory can persist between sessions, it is possible to continue the conversation within a use case with its history intact. This is part of achieving real persistent memory, where key information about a case does not disappear between sessions. This is ideal in collaborative software projects where continuity of knowledge is crucial.

##### Summary, decision log, and conversation traceability

As agents (and the human PO) discuss each use case, it is essential to manage the history intelligently. A critical component is generating periodic summaries of the conversation. Summaries condense the key points and essential context, drastically reducing the amount of text that must be kept in the model’s context window. This allows the model to stay on track even for long conversations and avoid token limits. The solution is to summarize: compress the history into a coherent recap that preserves important decisions and details without overloading memory. In this way, each time a new message or round of discussion is added, the system can update the summary instead of appending the entire raw history. This ensures that the dialog remains coherent and that prior decisions remain accessible to agents, maintaining context in a manageable form.

Alongside summaries, it is crucial to keep an explicit log of decisions made. Whenever agents reach an agreement (for example, the choice of an architecture, a bug fix, the acceptance of a design), that decision should be recorded in the context (and possibly in a separate decision log). These decisions can be stored as facts or conclusions within the use case’s memory. Thus, at the end of the conversation (or at any time), a final report can be generated containing: (a) a narrative summary of the dialog and (b) a list of agreed decisions and conclusions. This not only helps any human reviewer understand what was decided, but also serves as a knowledge base for future queries. For example, if later a question arises about why a certain technology was chosen, the answer could be found by searching for that decision in the log.

Conversation traceability means being able to follow the trail of who said what and why throughout development. To achieve this, we need to store the message history with metadata: each recorded message should include the timestamp, role (e.g., PO, Backend Developer, QA Agent, etc.), and possibly the stage or subtask during which it occurred. This structure enables later filtering or search. It is useful to implement the ability to search for solutions proposed by role; for example, being able to ask, “What alternatives did the Architecture agent propose regarding caching?” and obtain the relevant conversation snippets.

Technically, this can be accomplished by indexing the content of conversations (for example, in a vector store for semantic search, or even as plain text with role-based filters). An advanced approach would store each message in a knowledge base with fields such as {content, role, use case, stage, date}, facilitating specific queries. Likewise, orchestration solutions often offer debugging tools that allow you to see what input each agent received, what output it generated, and how control passed to the next step—precisely to facilitate decision tracing in complex collaborations.

To instrument traceability, we can take inspiration from how frameworks handle state and messages. For example, Botpress suggests encapsulating each message with contextual information: who sends it, where it comes from in the flow, how it should be used (whether it is an instruction, a response, a decision), etc. In our system, annotating each exchange with the emitting role and perhaps a subtask identifier will provide this clarity. When the conversation concludes, we will have a complete documented thread that allows for auditing: one could reconstruct, step by step, how a certain solution was reached and who contributed each idea.

In summary, we will adopt a dual strategy: summarize and record. Summarize to control the size of the active context and keep agents focused, and record in detail so nothing is lost and to enable subsequent search and traceability. This combination enables long-term conversational memory without overwhelming the system in real time, while maintaining a single source of truth of everything that happened. It is worth noting that these practices also improve human collaboration: a new team member could read the summaries and decision logs of each use case to get up to speed quickly, rather than reading hundreds of unprocessed messages.

##### Organized simulation of a development team’s work: roles and sequence

For the platform to credibly and usefully simulate the work of a Software Engineering (SWE) team, it is necessary to establish a logical order in agent interactions, imitating the real flow of a development process. In essence, we will divide the work into stages, with defined roles for each stage, and coordinate the sequence in which they participate. This reflects both classical methodologies (e.g., a waterfall cycle of design–coding–testing) and agile practices with iterations, but always with a certain order (you cannot test before coding, for example). Below is a possible sequence of phases in the simulation with traceability and control:

**Use Case Creation (PO Input):** Everything begins when the Product Owner (human) defines a use case through the UI. Here the PO describes the desired functionality, acceptance criteria, constraints, etc. This input feeds the system and constitutes the initial specification in natural language. For example, the PO might say: “We need a user registration function with multi-factor authentication.” This step triggers the creation of a new conversation context for that use case (as mentioned earlier, isolated from others). No AI agent has intervened yet beyond receiving and confirming the input.

**Analysis/Design Phase:** In this phase, agents take on the roles of requirements analyst or architect. One or more agents (e.g., AnalystAgent, ArchitectAgent) discuss the given use case. If there are ambiguities or missing information, they could “ask” the PO for clarifications within the same context. Their objective is to convert the general request into more detailed requirements and technical solution proposals. For example, they could debate what architecture to use, whether a SQL or NoSQL database is appropriate, applicable design patterns, and so on. Here we could have one agent proposing a solution and another validating it (following the dual-role pattern). The output of this phase should be a summarized design document (added to the context): for example, “We decided to use an MVC architecture with a REST API in Node.js, a PostgreSQL database, etc., and these are the main components...” Technical tasks to implement are also identified.

**Coding (Implementation) Phase:** Once the high-level design is agreed upon, developer agents come into play. We can simulate different roles: FrontEndDevAgent, BackEndDevAgent, etc., depending on the use case. Following the design, these agents generate code (in natural language describing the code, or even pseudo-code) and discuss concrete implementation solutions among themselves. One agent can write a function and another (perhaps a code reviewer agent) reviews it, pointing out potential improvements or errors. This dialogue between “programmer” and “reviewer” is exactly what ChatDev implements in its coding stage, where a virtual programmer proposes code and another agent acts as a reviewer to verify and refine it. During this phase in our system, it would be useful for agents to reference decisions from the design stage (for example, if an authentication pattern was decided, to follow it). Since those decisions are persisted in context, agents can consult them directly. The coding phase can be subdivided into atomic subtasks (similar to sub-tickets or technical user stories). For example, implement the registration UI, implement email verification, etc., and assign (simulate) an agent or pair of agents to each subtask to maximize parallelism or clarity. However, even when subdivided, the flow must be coordinated so that everything eventually integrates. This is where a planner or controller can help: it can decide which sub-agent (or pair) acts next based on which parts of the code are ready. In multi-agent systems, there is often a planning agent that, after each step, evaluates the global state and activates the appropriate next agent.

In our context, the planner could be optional if we design a fixed sequence, but it is useful to handle iterations.

**Testing (Verification) Phase:** After (or even during) coding, the testing agent (QA) enters. This agent takes on the role of a tester or quality assurance engineer. Using the context (which includes what was implemented or described in previous steps), the QA agent validates the solution against the original acceptance criteria. For example, it could “test” basic cases (“What happens if the user enters an invalid password?”) and edge cases, looking for inconsistencies.

In our system, the tester agent could list failures or risks. An important point is iteration: if the tester finds issues, the simulation should feed back into the coding phase to fix them. This can be done by reactivating the developer agent with the list of found bugs in context. We will implement logic that prevents moving to the next stage until the testing agent “gives the green light” (or, in an agile context, a “done” criterion is met). This phase may also include a security or performance agent if we want to extend the simulation (for example, an agent that reviews code for vulnerabilities, etc., depending on the project scope).

**Documentation Phase:** Finally, once the functionality is implemented and validated, a documentation agent (e.g., DocumentationAgent or TechWriterAgent) generates useful documentation. This could include a user guide, code comments, or technical documentation for the team (depending on the purpose). In our simulation, the documentation agent would take the complete context (design, code, tests) and produce a final summary usable by humans: for example, a README section describing how the new feature works and how to deploy it. This closes the cycle for each use case, leaving as tangible results both the artifact (code, design) and its associated documentation.

**Iteration Management and new cases:** After documentation, the use case can be considered complete. If the project continues with another use case, a new cycle begins (new context). However, there may be cross-feedback: for example, while implementing use case B, it is discovered that something in case A must be adjusted. This happens in agile methodologies; we could simulate it by allowing agents to remember past use cases via global memory (searching a vector store of previous decisions, for example). Since we maintain persistence for each context, an agent could consult the global knowledge base to see whether a previous solution from another case is applicable. This adds realism, imitating a team that leverages past experience.

To coordinate this entire process, we need central orchestration. It can be a fixed flow (sequence 1→2→3→4→5 as above) or a coordinating agent that evaluates at each moment which phase is next. A hybrid option is to use a flow graph where certain events trigger transitions: e.g., “Design approved” → launch Coding; “Tests failed” → return to Coding with bug report; “Tests approved” → launch Documentation, etc. For simplicity, we could initially implement a state machine: Design state, then Implementation, then Testing, etc., with loops when a state fails (e.g., from Testing with errors back to Implementation).

It is important to emphasize that each agent role should have clear responsibilities and objectives, as in a human team. By defining, for example, an “Architect” agent with the mandate to approve or improve the “Programmer” agent’s design proposals, we emulate the internal review dynamic of a team. Structured collaboration (e.g., a debate between one agent and another on a decision) acts as a counterbalance that reduces errors (cross-examination between models). Our simulation will leverage this by assigning complementary roles that supervise each other in each phase (programmer/reviewer, developer/tester, etc.), which also increases traceability since each conclusion is the result of a deliberate dialogue.

In conclusion, to simulate the work of a SWE team in an orderly and intelligent way: (a) we divide the process into logical sequential phases, (b) we assign agents with specific roles in each phase (and sub-phase) that collaborate through dialogue, (c) we establish flow control mechanisms (planner or fixed sequence with conditions) to ensure order and manage iterations, and (d) we integrate everything with the context persistence layer already described (so that decisions and results from each stage feed the next and are stored). With this design, the system will act as a “virtual team” where shared memory (persistence) and structured communication enable development case by case, maintaining the project’s overall vision and remembering lessons learned in each iteration. This achieves not only solving use cases in isolation, but simulating the real collaborative process of a software engineering team, with its discussions, justified decisions, and finished products.



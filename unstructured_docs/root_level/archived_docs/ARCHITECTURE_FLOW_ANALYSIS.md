# SWE AI Fleet - Architecture Flow Analysis

## ğŸ¯ Current Architecture (As Implemented)

### ğŸ“Š Services Deployed

| Service | Port | Language | Status | Purpose |
|---------|------|----------|--------|---------|
| **Planning** | 50051 | Go | âœ… Running | FSM story lifecycle |
| **StoryCoach** | 50052 | Go | âœ… Running | Story scoring |
| **Workspace** | 50053 | Go | âœ… Running | Result scoring |
| **Context** | 50054 | Python | âœ… Running | Context hydration |
| **Orchestrator** | 50055 | Python | âœ… Running | Multi-agent coordination |
| **UI** | 80 | React | âœ… Running | Product Owner interface |

### ğŸ”„ Communication Patterns

#### 1. **Synchronous (gRPC)**

```
Gateway â†’ Orchestrator:50055 â†’ Context:50054
              â†“
          Deliberate(role, task)
              â†“
          Returns: winner + candidates
```

#### 2. **Asynchronous (NATS JetStream)**

```
Planning â†’ agile.events
             â†“
    OrchestratorPlanningConsumer
             â†“
    Derive subtasks (TO DO)
             â†“
    agent.requests
             â†“
    Workspace Runner (TO DO)
             â†“
    K8s Jobs
             â†“
    agent.responses
             â†“
    Context Service Consumer
```

#### 3. **Ray Jobs (Distributed Execution)**

```
Orchestrator.DeliberateAsync
    â†“
Creates N Ray jobs (VLLMAgentJob.remote)
    â†“
Ray Cluster executes in parallel
    â”œâ”€â†’ Agent 1 â†’ vLLM Server â†’ generate()
    â”œâ”€â†’ Agent 2 â†’ vLLM Server â†’ generate()
    â””â”€â†’ Agent 3 â†’ vLLM Server â†’ generate()
    â†“
Agents publish to NATS: agent.results.{task_id}
    â†“
DeliberationResultCollector
    â†“
Returns aggregated results
```

---

## ğŸ” Current Flow for Task Execution

### Scenario: "Add hello_world() function to utils.py"

#### **Path A: Sync Deliberation (Mock Agents)**

```
1. gRPC Call: Orchestrator.Deliberate
   {
     role: "DEV",
     task_description: "Add hello_world() function to src/utils.py",
     rounds: 1,
     num_agents: 3
   }

2. Orchestrator checks councils["DEV"]
   - If empty â†’ ERROR: "No agents configured"
   
3. Council executes deliberation
   Deliberate.execute(task, constraints)
   â”œâ”€â†’ Agent1.generate(task) â†’ proposal1
   â”œâ”€â†’ Agent2.generate(task) â†’ proposal2
   â””â”€â†’ Agent3.generate(task) â†’ proposal3
   
4. Peer review (if rounds > 1)
   â”œâ”€â†’ Agent1.critique(proposal2)
   â”œâ”€â†’ Agent2.critique(proposal3)
   â””â”€â†’ Agent3.critique(proposal1)
   
5. Scoring.score(proposals, rubric)
   â””â”€â†’ Ranked: [proposal1(0.95), proposal2(0.87), proposal3(0.82)]
   
6. Return DeliberateResponse
   {
     winner_id: "agent-dev-001",
     results: [...],
     duration_ms: 2500
   }
```

#### **Path B: Async Deliberation (Ray + vLLM)**

```
1. gRPC Call: Orchestrator.Deliberate (with async flag)
   OR Planning event consumed

2. DeliberateAsync.execute(task_id, task, role, num_agents)
   
3. Connect to Ray Cluster
   ray.init(address="ray://ray-head:10001")
   
4. Create N Ray Actors
   for i in range(num_agents):
       agent_actor = VLLMAgentJob.remote(
           agent_id=f"agent-{role}-{i}",
           vllm_url="http://vllm-server:8000",
           model="Qwen/Qwen3-0.6B",
           nats_url="nats://nats:4222"
       )
       
5. Submit jobs (non-blocking)
   job_refs = [
       agent_actor.run.remote(task_id, task, constraints)
       for agent in agents
   ]
   
6. Return immediately
   {
     task_id: "task-12345",
     status: "PENDING",
     agent_count: 3,
     ray_job_ids: [...]
   }
   
7. Ray executes jobs in parallel
   Each VLLMAgentJob:
   â”œâ”€â†’ Calls vLLM API
   â”œâ”€â†’ Generates proposal
   â”œâ”€â†’ Publishes to NATS: agent.results.task-12345
   â””â”€â†’ Job completes
   
8. DeliberationResultCollector (NATS consumer)
   â”œâ”€â†’ Collects agent.results.{task_id}
   â”œâ”€â†’ Waits for all N agents (with timeout)
   â”œâ”€â†’ Ranks results
   â””â”€â†’ Stores in memory cache
   
9. Client polls: GetDeliberationResult(task_id)
   {
     status: "COMPLETED",
     winner: {...},
     results: [...],
     duration_ms: 5000
   }
```

---

## âŒ **CRITICAL GAP: Tools Not Integrated**

### Current State
```python
# VLLMAgentJob.run() currently:
class VLLMAgentJob:
    def run(self, task_id, task_description, constraints):
        # 1. Call vLLM API
        prompt = f"Complete task: {task_description}"
        response = vllm_client.generate(prompt)
        
        # 2. Publish result to NATS
        nats.publish(f"agent.results.{task_id}", response)
```

**Problem**: Agent generates TEXT response, but doesn't EXECUTE code changes using tools!

### What's Missing

```python
# What we NEED:
class ToolEnabledAgentJob:
    def __init__(self, agent_id, role, workspace_path, tools):
        self.agent_id = agent_id
        self.role = role
        self.workspace_path = workspace_path
        
        # Initialize tools
        self.git = GitTool(workspace_path)
        self.files = FileTool(workspace_path)
        self.tests = TestTool(workspace_path)
    
    def run(self, task_id, task_description, constraints):
        # 1. Call LLM to get PLAN
        prompt = f"""Task: {task_description}
        Available tools: git, files, tests
        Generate step-by-step plan using tools.
        """
        plan = vllm_client.generate(prompt)
        
        # 2. EXECUTE plan using tools
        for step in plan:
            if step.tool == "files.write":
                self.files.write_file(step.path, step.content)
            elif step.tool == "git.commit":
                self.git.commit(step.message)
            elif step.tool == "tests.pytest":
                result = self.tests.pytest()
                if not result.success:
                    # Retry or report failure
        
        # 3. Verify and publish result
        verification = self._verify_task_completion(task_description)
        nats.publish(f"agent.results.{task_id}", verification)
```

---

## ğŸ¯ Solution: E2E Flow with Tools

### Architecture Update

```
Orchestrator
    â†“
Create Workspace Container (K8s Job)
    â”œâ”€â†’ Clone repo
    â”œâ”€â†’ Initialize tools (git, files, tests)
    â””â”€â†’ /workspace ready
    â†“
Execute Agent with Tools
    â”œâ”€â†’ LLM generates plan
    â”œâ”€â†’ Agent uses FileTool.write_file()
    â”œâ”€â†’ Agent uses TestTool.pytest()
    â”œâ”€â†’ Agent uses GitTool.commit()
    â””â”€â†’ Workspace execution complete
    â†“
Publish Results to NATS
    â†“
Context Service updates Neo4j
    â†“
Task complete âœ…
```

### Implementation Plan

```
Phase 1: Standalone Tool Execution (Current PR) âœ…
- Tools implemented
- Unit tests passing
- Tools work in workspace container

Phase 2: Agent + Tools Integration (Next)
- Create ToolEnabledAgent class
- Agent calls tools during task execution
- E2E test: Agent uses tools to complete task

Phase 3: Workspace Runner Integration
- Workspace Runner creates K8s Job
- Job includes tools
- Agent executes within Job
- Results published to NATS

Phase 4: Full Flow
- Planning â†’ agile.events
- Orchestrator â†’ derives tasks
- Workspace Runner â†’ creates Jobs with tools
- Agent â†’ executes using tools
- Results â†’ Context Service
```

---

## ğŸš€ E2E Test Scenario (For This Session)

### Goal: Agent uses tools to add a function

```
1. Test creates workspace
   /workspace/
     /src/utils.py    (existing file)
     /tests/          (existing tests)

2. Test calls: orchestrator.execute_task_with_tools()
   task = "Add hello_world() function to src/utils.py"
   role = "DEV"

3. Orchestrator dispatches to ToolEnabledMockAgent
   agent = ToolEnabledMockAgent(
       agent_id="agent-dev-001",
       workspace="/workspace",
       tools=[GitTool, FileTool, TestTool]
   )

4. Agent executes task:
   Step 1: files.read_file("src/utils.py")
   Step 2: files.append_file("src/utils.py", new_function_code)
   Step 3: tests.pytest() to verify
   Step 4: git.add(["src/utils.py"])
   Step 5: git.commit("feat: add hello_world()")

5. Agent returns result:
   {
     success: true,
     operations: [read, append, pytest, add, commit],
     audit_trail: [...],
     final_state: "committed"
   }

6. Test verifies:
   âœ“ Function added to file
   âœ“ Tests pass
   âœ“ Commit created
   âœ“ Audit trail complete
```

---

## ğŸ”§ Required Components

### 1. ToolEnabledMockAgent

```python
# src/swe_ai_fleet/orchestrator/domain/agents/tool_enabled_mock_agent.py

from swe_ai_fleet.tools import GitTool, FileTool, TestTool

class ToolEnabledMockAgent(Agent):
    """Mock agent that can use tools to complete tasks."""
    
    def __init__(self, agent_id, role, workspace_path):
        super().__init__(agent_id, role)
        self.workspace_path = workspace_path
        
        # Initialize tools
        self.git = GitTool(workspace_path)
        self.files = FileTool(workspace_path)
        self.tests = TestTool(workspace_path)
    
    def generate(self, task, constraints, diversity=False):
        """Generate solution by EXECUTING task with tools."""
        # Parse task
        if "add" in task.lower() and "function" in task.lower():
            return self._add_function_task(task)
        elif "fix" in task.lower() and "bug" in task.lower():
            return self._fix_bug_task(task)
        else:
            return {"content": f"Mock solution for: {task}"}
    
    def _add_function_task(self, task):
        """Execute task to add a function."""
        # 1. Read file
        result = self.files.read_file("src/utils.py")
        if not result.success:
            return {"error": "Cannot read file"}
        
        # 2. Add function
        new_code = '''

def hello_world():
    """Return greeting."""
    return "Hello, World!"
'''
        append = self.files.append_file("src/utils.py", new_code)
        
        # 3. Run tests
        test_result = self.tests.pytest()
        
        # 4. Commit if tests pass
        if test_result.success:
            self.git.add(["src/utils.py"])
            self.git.commit("feat: add hello_world() function")
            
            return {
                "content": "Successfully added hello_world() function",
                "tests_passed": True,
                "committed": True
            }
        else:
            return {
                "content": "Added function but tests failed",
                "tests_passed": False
            }
```

### 2. K8s Job Template

```yaml
# tests/e2e/agent_tooling/k8s-agent-task-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: agent-task-{{ task_id }}
  namespace: swe-ai-fleet
spec:
  template:
    spec:
      containers:
      - name: agent-workspace
        image: registry.underpassai.com/swe-fleet/agent-workspace:latest
        command: ["/bin/bash", "-c"]
        args:
          - |
            # Clone repo
            git clone {{ repo_url }} /workspace
            cd /workspace
            
            # Initialize Python environment
            source .venv/bin/activate
            
            # Execute agent task with tools
            python -m swe_ai_fleet.orchestrator.agent_executor \
              --task-id {{ task_id }} \
              --role {{ role }} \
              --task "{{ task_description }}" \
              --workspace /workspace \
              --nats-url nats://nats:4222
        
        volumeMounts:
          - name: workspace
            mountPath: /workspace
        
        env:
          - name: TASK_ID
            value: "{{ task_id }}"
          - name: AGENT_ROLE
            value: "{{ role }}"
          - name: NATS_URL
            value: "nats://nats:4222"
      
      volumes:
        - name: workspace
          emptyDir: {}
      
      restartPolicy: Never
```

---

## ğŸ¯ E2E Test Implementation

### Objective
Test complete flow: Orchestrator â†’ Agent â†’ Tools â†’ Task Completion

### Current Limitations
1. âŒ VLLMAgentJob doesn't have access to tools
2. âŒ MockAgent doesn't use tools
3. âŒ No workspace container integration yet

### Solution for This Session

**Simplified E2E**: Direct tool usage by test (simulating agent behavior)

```python
# tests/e2e/agent_tooling/test_orchestrator_agent_tools_e2e.py

@pytest.mark.e2e
def test_agent_completes_coding_task_with_tools():
    """
    E2E: Simulated agent uses tools to complete task.
    
    Simulates what a real agent would do:
    1. Receive task from Orchestrator
    2. Use FileTool to read/modify code
    3. Use TestTool to verify
    4. Use GitTool to commit
    5. Report completion
    """
    
    # Setup workspace (like K8s Job would)
    with tempfile.TemporaryDirectory() as tmpdir:
        workspace = Path(tmpdir)
        
        # Initialize workspace
        setup_test_workspace(workspace)
        
        # === SIMULATE AGENT EXECUTION ===
        
        # Agent receives task (from Orchestrator)
        task = {
            "task_id": "task-001",
            "role": "DEV",
            "description": "Add hello_world() function to src/utils.py"
        }
        
        # Agent initializes tools
        git = GitTool(workspace)
        files = FileTool(workspace)
        tests = TestTool(workspace)
        
        # Agent completes task
        result = complete_coding_task(task, git, files, tests)
        
        # Verify
        assert result["success"]
        assert result["tests_passed"]
        assert result["committed"]
        
        # Would publish to NATS in real scenario
        # nats.publish(f"agent.results.{task['task_id']}", result)
```

---

## ğŸ”§ What Needs to be Built

### Immediate (This Session)
1. âœ… Tools implemented (git, file, test, docker, http, db)
2. âœ… Unit tests for tools
3. âœ… E2E test simulating agent tool usage
4. â³ K8s Job to run E2E test in cluster

### Short-term (Next Sprint)
1. â³ ToolEnabledAgent class
2. â³ Integration with VLLMAgentJob
3. â³ Workspace Runner that creates K8s Jobs
4. â³ agent-executor script for Job container

### Medium-term
1. â³ Tool Gateway (FastAPI) for unified API
2. â³ Policy Engine (RBAC) for tool access
3. â³ LLM-based tool selection (agent decides which tools to use)

---

## ğŸ“Š Current vs Target Architecture

### Current (Implemented)
```
Orchestrator
    â†“
MockAgent.generate()
    â†“
Returns text proposal
    â†“
No actual code execution
```

### Target (This Session's Goal)
```
Orchestrator
    â†“
ToolEnabledAgent.generate()
    â†“
Uses FileTool, GitTool, TestTool
    â†“
Executes actual code changes
    â†“
Returns execution result + audit
```

### Future (Full System)
```
Planning
    â†“ NATS: agile.events
Orchestrator Consumer
    â†“ Derives tasks
Workspace Runner
    â†“ Creates K8s Job
Agent Container (with tools)
    â”œâ”€â†’ vLLM generates plan
    â”œâ”€â†’ Tools execute plan
    â”œâ”€â†’ Verify completion
    â””â”€â†’ Publish to NATS
Context Service
    â†“ Updates Neo4j
Task Complete âœ…
```

---

## ğŸ¯ Next Steps for E2E Test

1. **Run E2E test locally** âœ… DONE
   - Tools work in temp workspace
   - Agent workflow simulated
   - 5 tests passing

2. **Create K8s Job manifest** âœ… DONE
   - Job clones repo
   - Installs dependencies
   - Runs E2E tests
   - Reports results

3. **Execute in Kubernetes cluster** â³ NEXT
   - Apply Job to cluster
   - Monitor execution
   - Verify tools work in K8s
   - Check audit trail

4. **Integrate with Orchestrator** (Future)
   - ToolEnabledAgent
   - Workspace Runner
   - Full async flow

---

**Status**: Ready to execute E2E in Kubernetes cluster! ğŸš€

The tools are implemented, tests pass locally, and K8s Job is configured.
Next: Deploy to cluster and verify agent tooling works in production environment.


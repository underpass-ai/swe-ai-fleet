# DiseÃ±o de IntegraciÃ³n: Deliberate & Orchestrate

## ðŸŽ¯ Objetivo

Integrar los casos de uso **Deliberate** y **Orchestrate** en los consumers del Orchestrator Service para crear un flujo completo de ejecuciÃ³n de tareas con multi-agent deliberation.

---

## ðŸ“ Arquitectura Actual de Use Cases

### 1. **Deliberate** - DeliberaciÃ³n Entre Pares

```python
class Deliberate:
    def __init__(
        self, 
        agents: list[Agent],      # Agentes que participan
        tooling: Scoring,         # Sistema de scoring
        rounds: int = 1           # Rondas de peer review
    ):
        pass
    
    def execute(
        self, 
        task: str,                # DescripciÃ³n de la tarea
        constraints: TaskConstraints  # Rubric y restricciones
    ) -> list[DeliberationResult]:  # Ordenados por score (alto â†’ bajo)
        """
        Proceso:
        1. Cada agente genera propuesta inicial
        2. Peer review: cada agente critica la propuesta del siguiente
        3. RevisiÃ³n: cada agente mejora su propuesta con feedback
        4. Scoring: evalÃºa todas las propuestas
        5. Retorna lista ordenada por score
        """
```

**Entrada**:
- `task`: DescripciÃ³n textual de lo que hay que hacer
- `constraints`: Rubric para evaluaciÃ³n, requirements, timeouts

**Salida**:
```python
DeliberationResult:
  - proposal: Proposal (author: Agent, content: str)
  - checks: CheckSuite (policy, lint, dryrun)
  - score: float
  - rank: int
```

---

### 2. **Orchestrate** - Workflow Completo

```python
class Orchestrate:
    def __init__(
        self,
        config: SystemConfig,
        councils: dict[str, Deliberate],  # role â†’ Deliberate instance
        architect: ArchitectSelectorService  # Selector final
    ):
        pass
    
    def execute(
        self,
        role: Role,               # Rol que maneja la tarea
        task: Task,               # Objeto Task del dominio
        constraints: TaskConstraints
    ) -> dict[str, Any]:
        """
        Proceso:
        1. Selecciona council basado en role
        2. Ejecuta deliberaciÃ³n (llama Deliberate.execute)
        3. Architect selecciona el mejor de los mejores
        4. Retorna {winner: DeliberationResult, candidates: list}
        """
```

**Entrada**:
- `role`: Role enum (DEV, QA, ARCHITECT, etc.)
- `task`: Task object con descripciÃ³n, constraints, metadata
- `constraints`: TaskConstraints con rubrics

**Salida**:
```python
{
    "winner": DeliberationResult,      # El elegido por architect
    "candidates": list[DeliberationResult]  # Otros top-k
}
```

---

## ðŸ”„ Flujo de Datos Completo: De Planning a Execution

### Fase 1: Plan Approved â†’ Task Derivation

```
Planning Service
    â†“
  publishes: planning.plan.approved
    {story_id, plan_id, roles: [DEV, QA], ...}
    â†“
OrchestratorPlanningConsumer
    â†“
  derive_subtasks()  # â† FALTA IMPLEMENTAR
    â†“
  list[Task]
    {
      id: "TASK-001",
      description: "Implement login endpoint",
      role: DEV,
      constraints: {...},
      dependencies: [...]
    }
    â†“
  TaskQueue.enqueue(task)  # â† FALTA IMPLEMENTAR
```

### Fase 2: Task Dispatch â†’ Orchestration

```
TaskQueue.dequeue()
    â†“
  next_task: Task
    â†“
Orchestrate.execute(role=DEV, task=next_task, constraints)
    â†“
  council = councils["DEV"]  # Deliberate instance para DEV
    â†“
  Deliberate.execute(task.description, constraints)
    â†“
    [Agent1.generate(), Agent2.generate(), Agent3.generate()]
      â†“ peer review rounds
    [Proposal1, Proposal2, Proposal3]
      â†“ scoring
    [Result1(score=0.95), Result2(score=0.87), Result3(score=0.82)]
    â†“
  Architect.choose([Result1, Result2, Result3])
    â†“
  {winner: Result1, candidates: [Result2, Result3]}
    â†“
  Publicar: orchestration.task.dispatched
    {
      story_id,
      task_id: "TASK-001",
      agent_id: "agent-dev-001",
      role: "DEV"
    }
```

### Fase 3: Agent Execution â†’ Response Processing

```
Agent ejecuta task en workspace
    â†“
  publishes: agent.response.completed
    {
      task_id: "TASK-001",
      agent_id: "agent-dev-001",
      output: {...},
      checks_passed: true
    }
    â†“
OrchestratorAgentResponseConsumer
    â†“
  if requires_deliberation:
    # Caso: mÃºltiples agentes trabajaron en paralelo
    â†“
    Deliberate.execute(...)  # â† Comparar resultados
    â†“
    winner = results[0]
    â†“
    Publicar: orchestration.deliberation.completed
      {
        story_id,
        task_id,
        decisions: [{id, type, rationale, ...}]
      }
  â†“
  TaskQueue.mark_completed(task_id)
  â†“
  next_task = TaskQueue.dequeue()
  â†“
  if next_task:
    Orchestrate.execute(...)  # Siguiente tarea
```

---

## ðŸ—ï¸ Componentes Necesarios

### 1. âŒ **TaskQueue** (Redis/Valkey)

```python
class TaskQueue:
    """Cola de tareas con prioridades y dependencias."""
    
    def __init__(self, redis_client: Redis):
        self.redis = redis_client
        self.queue_key = "orchestrator:task_queue"
        self.status_key = "orchestrator:task_status"
    
    async def enqueue(self, task: Task, priority: int = 0):
        """Agregar tarea a la cola con prioridad."""
        task_data = {
            "id": task.id,
            "story_id": task.story_id,
            "description": task.description,
            "role": task.role.name,
            "constraints": task.constraints.to_dict(),
            "dependencies": task.dependencies,
            "priority": priority,
            "created_at": time.time(),
        }
        
        # Usar sorted set para prioridades
        await self.redis.zadd(
            self.queue_key,
            {json.dumps(task_data): priority}
        )
        
        # Set status
        await self.redis.hset(
            self.status_key,
            task.id,
            "QUEUED"
        )
    
    async def dequeue(self) -> Task | None:
        """Obtener siguiente tarea (mayor prioridad)."""
        # ZPOPMAX obtiene el de mayor score (prioridad)
        result = await self.redis.zpopmax(self.queue_key, count=1)
        if not result:
            return None
        
        task_json, priority = result[0]
        task_data = json.loads(task_json)
        
        # Update status
        await self.redis.hset(
            self.status_key,
            task_data["id"],
            "IN_PROGRESS"
        )
        
        return Task.from_dict(task_data)
    
    async def mark_completed(self, task_id: str):
        """Marcar tarea como completada."""
        await self.redis.hset(
            self.status_key,
            task_id,
            "COMPLETED"
        )
    
    async def mark_failed(self, task_id: str, error: str):
        """Marcar tarea como fallida."""
        await self.redis.hset(
            self.status_key,
            task_id,
            f"FAILED:{error}"
        )
    
    async def get_status(self, task_id: str) -> str:
        """Obtener status de una tarea."""
        status = await self.redis.hget(self.status_key, task_id)
        return status.decode() if status else "UNKNOWN"
```

---

### 2. âŒ **derive_subtasks()** - DerivaciÃ³n de Tareas

```python
async def derive_subtasks(
    plan_id: str,
    plan_content: str,  # Contenido del plan (markdown/structured)
    roles: list[str]
) -> list[Task]:
    """
    Analiza un plan y extrae subtasks atÃ³micas ejecutables.
    
    Proceso:
    1. Parsear plan (markdown sections, YAML, o JSON)
    2. Identificar tasks por role
    3. Extraer dependencias entre tasks
    4. Crear objetos Task con constraints
    5. Asignar prioridades
    
    Returns:
        Lista de Tasks ordenadas por dependencias
    """
    
    # OpciÃ³n A: Plan es markdown con estructura
    # ## DEV Tasks
    # - [ ] TASK-001: Implement login endpoint
    #   - Depends on: TASK-000
    #   - Priority: HIGH
    
    # OpciÃ³n B: Plan es JSON/YAML estructurado
    # tasks:
    #   - id: TASK-001
    #     role: DEV
    #     description: "Implement login endpoint"
    #     dependencies: [TASK-000]
    
    tasks = []
    
    # Parse plan (ejemplo simplificado)
    plan_data = yaml.safe_load(plan_content)  # o markdown parser
    
    for task_def in plan_data.get("tasks", []):
        task = Task(
            id=task_def["id"],
            story_id=task_def["story_id"],
            plan_id=plan_id,
            description=task_def["description"],
            role=Role[task_def["role"]],
            constraints=TaskConstraints.from_dict(task_def.get("constraints", {})),
            dependencies=task_def.get("dependencies", []),
            priority=task_def.get("priority", 0),
        )
        tasks.append(task)
    
    # Ordenar topolÃ³gicamente por dependencias
    sorted_tasks = topological_sort(tasks)
    
    return sorted_tasks
```

**Alternativa**: El Planning Service ya podrÃ­a generar las subtasks y publicarlas como eventos separados.

---

### 3. âœ… **Councils Initialization** - Ya parcialmente implementado

El servidor ya tiene la estructura pero con councils vacÃ­os:

```python
# En server.py
class OrchestratorServiceServicer:
    def __init__(self, config: SystemConfig):
        self.councils = {}  # â† Actualmente vacÃ­o
        self.orchestrator = None  # â† No inicializado
```

**Necesitamos**:

```python
def _initialize_councils(self, config: SystemConfig) -> dict[str, Deliberate]:
    """
    Inicializa councils para cada role con sus agentes.
    
    PROBLEMA: Â¿De dÃ³nde vienen los agentes?
    
    Opciones:
    A) Mock agents para desarrollo
    B) Agentes remotos (Ray, Kubernetes pods)
    C) Agent registry service
    """
    councils = {}
    scoring = Scoring()
    
    for role_config in config.roles:
        # OPCIÃ“N A: Crear mock agents
        agents = [
            MockAgent(agent_id=f"agent-{role_config.name.lower()}-{i}", 
                     role=Role[role_config.name])
            for i in range(role_config.replicas)
        ]
        
        # OPCIÃ“N B: Registrar agentes remotos (Ray actors)
        # agents = await self._get_agents_from_ray(role_config.name)
        
        # OPCIÃ“N C: Obtener de registry
        # agents = await self.agent_registry.get_agents(role_config.name)
        
        councils[role_config.name] = Deliberate(
            agents=agents,
            tooling=scoring,
            rounds=1  # Configurable
        )
    
    return councils
```

---

## ðŸ¤” Decisiones de DiseÃ±o CrÃ­ticas

### DecisiÃ³n 1: Â¿DÃ³nde se crean las subtasks?

**OpciÃ³n A**: Planning Service genera subtasks
- âœ… Planning tiene el contexto completo del plan
- âœ… Puede usar LLM para derivar tasks inteligentemente
- âœ… Orchestrator solo consume tasks ya definidas
- âŒ Acoplamiento: Planning debe conocer formato de Task

**OpciÃ³n B**: Orchestrator deriva subtasks del plan
- âœ… Orchestrator tiene control total sobre task decomposition
- âœ… Planning solo publica plan aprobado
- âŒ Orchestrator necesita entender formato de planes
- âŒ MÃ¡s complejo

**RecomendaciÃ³n**: **OpciÃ³n A** - Planning genera subtasks y las publica como eventos individuales.

---

### DecisiÃ³n 2: Â¿CuÃ¡ndo se ejecuta Deliberate?

**OpciÃ³n A**: Siempre delibera antes de dispatch
- âœ… Consistente, todas las tasks pasan por deliberaciÃ³n
- âŒ Lento, incluso para tasks triviales
- âŒ Desperdicia recursos

**OpciÃ³n B**: Delibera solo despuÃ©s de ejecuciÃ³n (post-hoc)
- âœ… RÃ¡pido para tasks simples
- âœ… DeliberaciÃ³n solo cuando hay mÃºltiples propuestas
- âŒ MÃ¡s complejo, requiere colectar resultados

**OpciÃ³n C**: Configurable por task
- âœ… Flexible
- âœ… Tasks crÃ­ticas siempre deliberan
- âœ… Tasks simples skip deliberation
- âŒ Requiere metadata adicional

**RecomendaciÃ³n**: **OpciÃ³n C** - Metadata en Task: `requires_pre_deliberation: bool`

---

### DecisiÃ³n 3: Â¿DÃ³nde viven los Agents?

**OpciÃ³n A**: Mock agents en memoria
- âœ… FÃ¡cil para desarrollo y testing
- âŒ No escalable
- âŒ No realistic

**OpciÃ³n B**: Ray remote actors
- âœ… Distribuido, escalable
- âœ… Ya tenemos Ray en el stack
- âŒ Requiere Ray cluster operacional
- âŒ Complejidad de networking

**OpciÃ³n C**: Kubernetes pods (agent runners)
- âœ… Ya tenemos workspace runners
- âœ… Isolation, security
- âŒ Startup time
- âŒ Resource overhead

**OpciÃ³n D**: Agent registry service + dynamic dispatch
- âœ… Flexible, puede usar Ray o K8s
- âœ… AbstracciÃ³n limpia
- âŒ Componente adicional

**RecomendaciÃ³n**: **Fase 1: A (mocks)**, **Fase 2: D (registry) + C (K8s runners)**

---

### DecisiÃ³n 4: Â¿CÃ³mo manejar dependencias entre tasks?

**OpciÃ³n A**: TaskQueue respeta dependencias
- âœ… Centralizado en un lugar
- âœ… FÃ¡cil de entender
- âŒ TaskQueue se vuelve complejo

**OpciÃ³n B**: Orchestrator verifica dependencias antes de dispatch
- âœ… LÃ³gica de dominio en Orchestrator
- âœ… TaskQueue se mantiene simple
- âŒ Requiere consultar estado de otras tasks

**RecomendaciÃ³n**: **OpciÃ³n B** - Orchestrator verifica, TaskQueue solo almacena

---

## ðŸŽ¯ Plan de ImplementaciÃ³n por Fases

### ðŸŸ¢ **Fase 1: Mock Implementation** (Para Testing)

**Objetivo**: Probar el flujo sin agentes reales

```python
# 1. MockAgent simple
class MockAgent(Agent):
    def generate(self, task, constraints, diversity=False):
        return {"content": f"Mock solution for {task}"}
    
    def critique(self, content, rubric):
        return "Mock feedback"
    
    def revise(self, content, feedback):
        return content  # Sin cambios

# 2. Inicializar councils con mocks
councils = {
    "DEV": Deliberate(
        agents=[MockAgent(f"dev-{i}") for i in range(3)],
        tooling=Scoring(),
        rounds=1
    )
}

# 3. TaskQueue simple (en memoria, sin Redis)
class InMemoryTaskQueue:
    def __init__(self):
        self.queue = []
    
    async def enqueue(self, task):
        self.queue.append(task)
    
    async def dequeue(self):
        return self.queue.pop(0) if self.queue else None
```

**Beneficio**: Podemos testear el flujo completo end-to-end sin infraestructura compleja.

---

### ðŸŸ¡ **Fase 2: Redis TaskQueue** (ProducciÃ³n)

**Objetivo**: Queue persistente y distribuido

```python
# Implementar TaskQueue con Redis como mostrado arriba
# - Prioridades
# - Estado persistente
# - Failover
```

---

### ðŸŸ¡ **Fase 3: Real Agents** (IntegraciÃ³n con Runners)

**Objetivo**: Conectar con workspace runners reales

```python
# AgentProxy que comunica con workspace runner via gRPC
class WorkspaceAgentProxy(Agent):
    def __init__(self, workspace_service_url):
        self.workspace = WorkspaceServiceStub(
            grpc.insecure_channel(workspace_service_url)
        )
    
    def generate(self, task, constraints, diversity=False):
        response = self.workspace.ExecuteTask(
            ExecuteTaskRequest(
                task=task,
                constraints=constraints
            )
        )
        return {"content": response.output}
```

---

### ðŸ”´ **Fase 4: Full Event Sourcing** (Futuro)

**Objetivo**: Todas las decisiones como eventos inmutables

- Cada deliberation result â†’ evento
- Cada task state change â†’ evento
- Replay para debugging
- Time travel queries

---

## ðŸ“ PrÃ³ximos Pasos Inmediatos

### 1. **Implementar Fase 1 (Mocks)** âœ… RECOMENDADO AHORA

```bash
# Archivos a crear/modificar:
1. src/swe_ai_fleet/orchestrator/domain/agents/mock_agent.py
2. src/swe_ai_fleet/orchestrator/adapters/in_memory_task_queue.py
3. services/orchestrator/server.py (inicializar councils con mocks)
4. services/orchestrator/consumers/planning_consumer.py (usar Orchestrate)
5. services/orchestrator/consumers/agent_response_consumer.py (usar Deliberate)
```

### 2. **Tests de IntegraciÃ³n**

```bash
# Testear flujo completo:
1. Planning publica plan.approved
2. Orchestrator deriva tasks (mock)
3. Orchestrator ejecuta Orchestrate
4. Verifica que winner es seleccionado
5. Verifica que task.dispatched es publicado
```

### 3. **Implementar TaskQueue con Redis** (Fase 2)

DespuÃ©s de que funcione con mocks.

---

## ðŸŽ¨ Diagrama de Secuencia Completo

```
Planning     Orchestrator     TaskQueue     Deliberate     Agents     Context
   â”‚               â”‚              â”‚             â”‚           â”‚          â”‚
   â”‚â”€â”€plan.approvedâ”€â”€>â”‚              â”‚             â”‚           â”‚          â”‚
   â”‚               â”‚â”€â”€enqueueâ”€â”€â”€>â”‚             â”‚           â”‚          â”‚
   â”‚               â”‚<â”€â”€okâ”€â”€â”€â”€â”€â”€â”€â”€â”‚             â”‚           â”‚          â”‚
   â”‚               â”‚â”€â”€dequeueâ”€â”€>â”‚             â”‚           â”‚          â”‚
   â”‚               â”‚<â”€taskâ”€â”€â”€â”€â”€â”€â”‚             â”‚           â”‚          â”‚
   â”‚               â”‚                          â”‚           â”‚          â”‚
   â”‚               â”‚â”€â”€execute(task)â”€â”€â”€â”€â”€â”€â”€â”€>â”‚           â”‚          â”‚
   â”‚               â”‚                          â”‚â”€â”€generate()â”€â”€>â”‚          â”‚
   â”‚               â”‚                          â”‚<â”€proposalâ”€â”€â”€â”€â”€â”‚          â”‚
   â”‚               â”‚                          â”‚ (x3 agents)   â”‚          â”‚
   â”‚               â”‚                          â”‚â”€â”€score()â”€â”€â”€â”€â”€â”€â”‚          â”‚
   â”‚               â”‚<â”€ranked resultsâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚           â”‚          â”‚
   â”‚               â”‚                          â”‚           â”‚          â”‚
   â”‚               â”‚â”€â”€publish: task.dispatchedâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
   â”‚               â”‚                                                 â”‚
   â”‚               â”‚                          Agent Workspace        â”‚
   â”‚               â”‚                          executes task          â”‚
   â”‚               â”‚                                â”‚                â”‚
   â”‚               â”‚<â”€â”€agent.response.completedâ”€â”€â”€â”€â”€â”‚                â”‚
   â”‚               â”‚                                                 â”‚
   â”‚               â”‚â”€â”€deliberate(results)â”€â”€â”€>â”‚           â”‚          â”‚
   â”‚               â”‚<â”€winnerâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚           â”‚          â”‚
   â”‚               â”‚                                                 â”‚
   â”‚               â”‚â”€â”€publish: deliberation.completedâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
   â”‚               â”‚                                                 â”‚
   â”‚               â”‚<â”€â”€context.updatedâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
```

---

Â¿Empezamos con la **Fase 1 (Mock Implementation)** para tener el flujo funcionando end-to-end?


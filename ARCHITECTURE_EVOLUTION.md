# Architecture Evolution: How the Vision Shaped Every Decision

## ğŸ‘¤ Creator

**Tirso** - Founder & Software Architect

This document traces how the architectural visionâ€”conceived by Tirso on August 9, 2025â€”evolved through intentional design decisions into a production-grade system by October 25, 2025.

---

## ğŸ¯ The Unifying Thesis

```
Precision Context + Small Agents + Deliberation = Production AI

This wasn't invented mid-project. This was commit #0's DNA (August 9, 2025).
```

This single principle, articulated in **RFC-0002** (August 16, 2025 - just 7 days after project start), shaped:
- **Technology Choices** (Neo4j, Redis, Ray)
- **Architectural Patterns** (Hexagonal, Microservices, CQRS)
- **Design Decisions** (Agents, Ports, Adapters)
- **Testing Strategy** (Unit/Integration/E2E pyramid)
- **Infrastructure** (Kubernetes, GPU scaling, NATS messaging)

---

## ğŸ“Š Evolution Map: Thesis â†’ Architecture

### Layer 1: Domain Theory (RFC-0002, August 16, 2025)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Problem: Small LLMs can't handle massive context â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Solution: Surgical context assembly              â”‚
â”‚ â”œâ”€ Per-use-case isolation (scoped memory)       â”‚
â”‚ â”œâ”€ Persistent decision logging                  â”‚
â”‚ â”œâ”€ Role-based conversation indexing             â”‚
â”‚ â””â”€ Reuse across similar cases                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Architectural Implication**: Need a **context assembly engine** that:
- Understands relationships (Neo4j)
- Scores relevance per role
- Assembles precise packs
- Persists for reuse (Redis)

---

### Layer 2: Orchestration Pattern (RFC-0003, August 16, 2025)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Five-Phase Orchestration with Human Oversight  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Use Case Creation (human input)             â”‚
â”‚ 2. Agent Council Proposes (deliberation)       â”‚
â”‚ 3. Human Validates (approval gate)             â”‚
â”‚ 4. Controlled Execution (with checkpoints)     â”‚
â”‚ 5. Closure & Learning Capture                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Architectural Implication**: Need an **orchestrator** that:
- Coordinates multi-agent consensus (not single-pass)
- Respects human checkpoints
- Logs every decision (for reuse)
- Isolates failures (one agent fails â‰  whole system fails)

---

### Layer 3: Clean Architecture (Hexagonal Pattern)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Domain Layer (Core Logic)          â”‚
â”‚  - Entities: Agent, Task, DeliberationResult â”‚
â”‚  - Use Cases: Orchestrate, Deliberate        â”‚
â”‚  - NO external dependencies                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ depends on
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Ports (Interfaces)â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ AgentPort           â”‚
        â”‚ ContextPort         â”‚
        â”‚ MessagingPort       â”‚
        â”‚ GraphQueryPort      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ implemented by
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Adapters (Implementations)â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ VLLMAgent (LLM)           â”‚
        â”‚ NATSAdapter (messaging)    â”‚
        â”‚ Neo4jAdapter (graph)       â”‚
        â”‚ RedisAdapter (cache)       â”‚
        â”‚ RayAdapter (distribution)  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why?** Because the thesis requires:
- âœ… **Testing small agents independently** (mock ports)
- âœ… **Swapping implementations** (Redis â†” PostgreSQL)
- âœ… **Clear contracts** (ports define what agents need)
- âœ… **Decoupling from frameworks** (domain stays pure)

---

### Layer 4: Microservices (Bounded Contexts)

#### The Problem With Monoliths
```
âŒ Monolithic Orchestrator
â”œâ”€ All code in one place
â”œâ”€ One deployment = all or nothing
â”œâ”€ Hard to test individual agents
â”œâ”€ Hard to scale selective components
â””â”€ Tight coupling between agent logic and gRPC/NATS
```

#### The Microservices Solution
```
âœ… Microservices = Orchestrated Hexagonal Services
â”œâ”€ Planning Service (FSM for story lifecycle)
â”œâ”€ StoryCoach Service (quality scoring)
â”œâ”€ Workspace Service (agent work validation)
â”œâ”€ Agent Orchestrator (deliberation coordination)
â””â”€ Each with: Domain (pure) + Ports (contracts) + Adapters (gRPC/NATS)
```

**Why?** Because the thesis requires:
- âœ… **Independent scaling** (CPU-bound scoring â‰  IO-bound messaging)
- âœ… **Domain isolation** (each service focuses on one responsibility)
- âœ… **Technology flexibility** (planning in Go, orchestration in Python)
- âœ… **Team autonomy** (services owned independently)

---

### Layer 5: Communication Pattern (NATS JetStream)

#### Why Not HTTP/REST?
```
âŒ Request/Response Pattern
â”œâ”€ Coupling: Requester must wait for response
â”œâ”€ Brittleness: Single point of failure
â”œâ”€ Ordering: No guarantee of message ordering
â””â”€ Replay: Can't replay history for debugging
```

#### Why NATS JetStream?
```
âœ… Event Streams with Persistence
â”œâ”€ Decoupling: Publish events, consumers handle async
â”œâ”€ Resilience: JetStream persists â†’ replay on recovery
â”œâ”€ Auditability: All events logged with timestamps
â”œâ”€ Ordering: Per-stream ordering guaranteed
â”œâ”€ Scalability: Multiple consumers without impact
â””â”€ Observability: Full event history available
```

**Why?** Because the thesis requires:
- âœ… **Async coordination** (agents work independently)
- âœ… **Audit trail** (every message = decision record)
- âœ… **Replay capability** (understand past decisions)
- âœ… **Resilience** (agent fails â‰  system fails)

---

### Layer 6: Data Persistence (Neo4j + Redis)

#### Why Neo4j for Context?
```
âœ… Knowledge Graph = Relationships + Scoring
â”œâ”€ Stores: Decisions, agents, tasks, dependencies
â”œâ”€ Queries: "What agents worked on feature X?"
â”œâ”€ Scoring: Rank relevance for context assembly
â”œâ”€ Replay: Walk decision path to understand "why"
â””â”€ Reuse: Find similar past cases
```

**Example Query**:
```cypher
MATCH (a:Agent)-[r:DELIBERATED]->(d:Decision)<-[:IMPLEMENTS]-(t:Task)
WHERE d.relevanceScore > 0.8 AND a.role = "QA"
RETURN d.description, d.rationale ORDER BY d.timestamp DESC
```

#### Why Redis for Scoped Memory?
```
âœ… Scoped Cache = Fast + Isolated
â”œâ”€ Per-use-case: Each task gets isolated namespace
â”œâ”€ TTL: Auto-cleanup when use case completes
â”œâ”€ Fast access: Sub-millisecond for context assembly
â”œâ”€ Serializable: Snapshot for debugging
â””â”€ Pub/Sub: Real-time notification of context changes
```

**Why?** Because the thesis requires:
- âœ… **Relationship awareness** (Neo4j shows HOW things relate)
- âœ… **Scoring capability** (Neo4j ranks relevance)
- âœ… **Isolation** (Redis namespace = use case boundary)
- âœ… **Performance** (Redis for hot context, Neo4j for cold analysis)

---

### Layer 7: Execution Engine (Ray + GPU Scaling)

#### The Challenge
```
Problem: Small agents need compute, but not all at once
â”œâ”€ Agent 1 generates proposal (GPU needed)
â”œâ”€ Agent 2 critiques (GPU needed)
â”œâ”€ Agent 3 revises (GPU needed)
â””â”€ Can't run all 3 in parallel on single GPU without queueing
```

#### The Solution: Ray with GPU Time-Slicing
```
âœ… Ray Cluster (KubeRay) with Time-Sliced GPUs
â”œâ”€ Head Node: No GPU (coordination only)
â”œâ”€ Worker Nodes: 8 virtual GPUs per physical GPU (time-sliced)
â”œâ”€ Queueing: Built-in work distribution
â”œâ”€ Auto-scaling: Add nodes = add capacity linearly
â””â”€ Fault-tolerance: Built-in retries + checkpointing
```

**Measured Performance**:
```
Hardware: 4Ã— RTX 3090 (24GB each) + time-slicing
Capacity: 8 concurrent deliberations
Time: ~60s per 3-agent deliberation
Success: 100% (5/5 production runs)
GPU Util: Verified working, scheduled preemption
```

**Why?** Because the thesis requires:
- âœ… **Horizontal scaling** (add GPUs = add capacity)
- âœ… **Efficient utilization** (time-slicing maximizes GPU use)
- âœ… **Cost predictability** (CapEx not OpEx)
- âœ… **Decoupling agents** (each agent = independent Ray task)

---

### Layer 8: Testing Strategy (Pyramid)

#### Why This Shape?
```
        E2E Tests
       /          \  â† Full system, slow (5+ min)
      /            \
     /   Integration \  â† With containers, medium (45s)
    /                 \
   /        Unit        \  â† Mocked ports, fast (<300ms each)
  /_____________________ \

Coverage Breakdown:
â”œâ”€ Unit Tests (fast)
â”‚  â”œâ”€ Mock all ports (AgentPort, ContextPort, MessagingPort)
â”‚  â”œâ”€ Validate domain logic in isolation
â”‚  â”œâ”€ Target: 90%+ new code coverage
â”‚  â””â”€ Example: test_orchestrate_with_mocked_agents
â”‚
â”œâ”€ Integration Tests (medium)
â”‚  â”œâ”€ Real adapters (NATS, Neo4j, Redis containers)
â”‚  â”œâ”€ Validate seams (domain + infrastructure)
â”‚  â”œâ”€ Example: test_orchestrate_with_real_nats
â”‚  â””â”€ Ensure message routing works
â”‚
â””â”€ E2E Tests (slow)
   â”œâ”€ Full Kubernetes cluster
   â”œâ”€ Real Ray workers + LLM models
   â”œâ”€ Validate complete workflows
   â””â”€ Example: test_deliberation_end_to_end
```

**Why?** Because the thesis requires:
- âœ… **Proof agents work** (unit tests with mocks)
- âœ… **Proof infrastructure works** (integration tests with containers)
- âœ… **Proof system works** (E2E tests in production environment)
- âœ… **Fast feedback** (unit tests in milliseconds)
- âœ… **Traceability** (each layer proves something specific)

---

## ğŸ”„ How Thesis â†’ Architecture â†’ Code

### Example: The Orchestrate Use Case

**From Thesis**:
> "Small agents need precise context + deliberation to achieve quality"

**Architecture Decision**:
```
Domain: OrchestrateUseCase (pure business logic)
  â”œâ”€ Accepts: TaskSpec + AgentPool + ContextPort + MessagingPort
  â”œâ”€ Does: Coordinate deliberation rounds (generate â†’ critique â†’ revise)
  â””â”€ Returns: WinningProposal + DecisionArtifacts

Ports: Define contracts
  â”œâ”€ AgentPort.generate() â†’ Proposal
  â”œâ”€ AgentPort.critique() â†’ Critique
  â”œâ”€ ContextPort.assemble() â†’ PreciseContext
  â””â”€ MessagingPort.publish_event() â†’ DomainEvent

Adapters: Implement ports
  â”œâ”€ VLLMAgentAdapter (real LLM with precise context)
  â”œâ”€ MockAgentAdapter (for unit testing)
  â”œâ”€ NATSMessagingAdapter (async event publishing)
  â””â”€ Neo4jContextAdapter (surgical context assembly)

Tests:
  â”œâ”€ Unit: Mock all ports, test deliberation logic
  â”œâ”€ Integration: Real NATS + Neo4j, test message routing
  â””â”€ E2E: Real Ray workers + LLM, test full flow
```

**Code Structure**:
```
services/orchestrator/
â”œâ”€â”€ domain/
â”‚   â”œâ”€â”€ entities/          â† Pure Python, zero dependencies
â”‚   â”‚   â”œâ”€â”€ agent.py       (Agent interface)
â”‚   â”‚   â”œâ”€â”€ task.py        (Task model)
â”‚   â”‚   â”œâ”€â”€ deliberation_result.py
â”‚   â”‚   â””â”€â”€ role.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ports/             â† Contracts (abstractions)
â”‚   â”‚   â”œâ”€â”€ agent_port.py
â”‚   â”‚   â”œâ”€â”€ context_port.py
â”‚   â”‚   â”œâ”€â”€ messaging_port.py
â”‚   â”‚   â””â”€â”€ ray_execution_port.py
â”‚   â”‚
â”‚   â””â”€â”€ usecases/          â† Pure business logic
â”‚       â”œâ”€â”€ orchestrate.py (coordinates agents)
â”‚       â””â”€â”€ deliberate.py  (manages peer review)
â”‚
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ adapters/          â† Implementations (external deps here)
â”‚   â”‚   â”œâ”€â”€ vllm_agent_adapter.py (real LLM)
â”‚   â”‚   â”œâ”€â”€ mock_agent_adapter.py (for testing)
â”‚   â”‚   â”œâ”€â”€ nats_messaging_adapter.py
â”‚   â”‚   â”œâ”€â”€ neo4j_context_adapter.py
â”‚   â”‚   â””â”€â”€ ray_executor_adapter.py
â”‚   â”‚
â”‚   â”œâ”€â”€ config/            â† Dependency injection
â”‚   â”‚   â””â”€â”€ container.py   (wires adapters)
â”‚   â”‚
â”‚   â””â”€â”€ handlers/          â† gRPC/HTTP endpoints
â”‚       â””â”€â”€ orchestrator_handler.py
â”‚
â””â”€â”€ tests/
    â”œâ”€â”€ unit/              â† Mock adapters
    â”‚   â”œâ”€â”€ test_orchestrate_usecase.py
    â”‚   â”œâ”€â”€ test_deliberate_usecase.py
    â”‚   â””â”€â”€ fixtures/mocks.py
    â”‚
    â”œâ”€â”€ integration/       â† Real adapters, containers
    â”‚   â”œâ”€â”€ test_orchestrate_with_nats.py
    â”‚   â””â”€â”€ docker-compose.yml
    â”‚
    â””â”€â”€ e2e/              â† Full system, K8s
        â””â”€â”€ test_deliberation_end_to_end.py
```

---

## ğŸ¯ Decision Trace: Why Each Technology

| Decision | Thesis Requirement | Why | Alternative Rejected |
|----------|-------------------|-----|----------------------|
| **Neo4j** | Precision context + relationships | Knowledge graph = understand relationships + score relevance | PostgreSQL (no relationships), MongoDB (no graph querying) |
| **Redis** | Scoped memory isolation | Per-use-case cache = fast, isolated | PostgreSQL (slower for hot cache), file system (lost on restart) |
| **NATS JetStream** | Async coordination + auditability | Persistent streams = replay history + audit trail | HTTP/REST (blocking, harder to debug), Kafka (overkill, more ops overhead) |
| **Ray** | Horizontal GPU scaling | Distributed compute + GPU scheduling built-in | Kubernetes jobs (manual scheduling), SLURM (not cloud-native) |
| **Hexagonal** | Testable small agents | Ports/Adapters = mock infrastructure, test logic | Monolith (can't test agents independently), Layered (can't swap adapters) |
| **Microservices** | Bounded contexts + independent scaling | Each service = one responsibility = one reason to change | Monolith (all-or-nothing), Service mesh (overkill) |
| **gRPC** | Efficient agent communication | Protocol Buffers = typed contracts + performance | REST (less efficient), GraphQL (overkill) |
| **Kubernetes** | Production ops + auto-scaling | Cloud-native deployment + declarative config | Docker Compose (dev-only), VMs (manual scaling) |

---

## ğŸ§  How To Trace Any Decision Back To The Thesis

**Question**: "Why is context scoped per-use-case?"
```
Answer: RFC-0002 (August 16) says small agents need focused context
  â†“ So we isolate memory per task (Redis namespace)
  â†“ And score relevance per use case (Neo4j)
  â†“ So agents get only what's relevant
  â†“ Result: Smaller context window = 7B model works
```

**Question**: "Why use NATS instead of HTTP?"
```
Answer: RFC-0003 (August 16) requires auditability + coordination
  â†“ NATS JetStream persists all events (auditability)
  â†“ Multiple consumers = no blocking (coordination)
  â†“ Replay capability for debugging (transparency)
  â†“ Result: Full decision trail + resilient orchestration
```

**Question**: "Why Hexagonal Architecture?"
```
Answer: Thesis needs testable agents
  â†“ Ports = abstract dependencies
  â†“ Unit tests mock ports (test agent logic)
  â†“ Integration tests use real adapters (test infrastructure)
  â†“ Result: 92% coverage with fast, reliable tests
```

---

## ğŸš€ How This Enables Future Decisions

### Scaling Agents
```
Thesis: Small agents + horizontal scaling
  â†“ Architecture: Ray workers independent
  â†“ Infrastructure: GPU time-slicing
  â†“ Future: Add agents to council âœ…
        Add GPUs to cluster âœ…
        Scale without code changes âœ…
```

### Adding New Domains
```
Thesis: Precision context = reusable across domains
  â†“ Architecture: Neo4j relationships agnostic to domain
  â†“ Infrastructure: NATS topics isolate domains
  â†“ Future: New use case = new contexts âœ…
          Knowledge base shared âœ…
          Same orchestrator reused âœ…
```

### Swapping LLM Models
```
Thesis: Small models work with precision context
  â†“ Architecture: AgentPort abstracts LLM implementation
  â†“ Infrastructure: VLLMAgentAdapter = pluggable
  â†“ Future: Qwen 7B â†’ Llama 13B âœ…
          Just update adapter âœ…
          No domain logic changes âœ…
```

---

## âœ¨ The Coherence Check

Every major system component should answer:

> **"How do you enable the thesis: Precision Context + Small Agents + Deliberation = Production AI?"**

| Component | Answer |
|-----------|--------|
| **Neo4j** | I store relationships so context assembly can score relevance |
| **Redis** | I isolate scoped memory per use case so agents stay focused |
| **NATS** | I coordinate async agents and log every decision for replay |
| **Ray** | I distribute agents across GPUs so small models scale horizontally |
| **Hexagonal** | I abstract infrastructure so agents can be tested independently |
| **Microservices** | I isolate bounded contexts so each service evolves independently |
| **Testing Pyramid** | I prove agents work at unit, integration, and E2E levels |
| **Kubernetes** | I orchestrate services at scale with declarative config |

**If a component can't answer this question, it's not aligned with the thesis.**

---

## ğŸ¬ How To Pitch This

**For Engineers**:
```
"Every line of code traces back to one principle: small agents with 
precise context work better than large models with massive context. 
Here's how we built that into the architecture in just 77 days (Aug 9 â†’ Oct 25)."
```

**For Architects**:
```
"We didn't choose technologies randomly. Each choice enables the core 
thesis. Neo4j for relationships, Redis for scoping, Ray for scaling.
See how they fit together?"
```

**For Investors**:
```
"This is consistent, defensible architecture. Not pivots or experiments.
77 days of aligned decisions building toward one goal: production-grade 
AI that works on consumer hardware. From RFC to production is our speed."
```

---

**Branch**: `docs/project-genesis`  
**Document Version**: 2.0 (corrected dates)  
**Last Updated**: 2025-10-25  
**Timeline**: August 9, 2025 â†’ October 25, 2025 (77 days)

# SWE AI Fleet - Project Rules

## ğŸ‘¤ Project Context

**Project Owner & Lead Architect**: Tirso
- **Role**: Software Architect and Creator of SWE AI Fleet
- **Expertise**: System architecture, microservices, AI/ML infrastructure
- **Context**: As the architect, Tirso has deep knowledge of:
  - Overall system design decisions and rationale
  - Microservices architecture and integration patterns
  - Infrastructure and deployment strategies
  - Project roadmap and priorities

**Interaction Guidelines**:
- Treat Tirso as the technical authority on architectural decisions
- Present options with trade-offs for architectural choices
- Assume high technical proficiency - can dive deep into implementation details
- Focus on production-ready, scalable solutions aligned with project vision
- Respect existing architectural patterns unless explicitly asked to propose alternatives

**Session Initialization Protocol**:
- At the start of EVERY new session, READ the project documentation:
  - `README.md` - Project overview and quick start
  - `ROADMAP.md` - Current project progress and milestones
  - `docs/architecture/MICROSERVICES_ARCHITECTURE.md` - System architecture
  - `docs/architecture/COMPONENT_INTERACTIONS.md` - Component communication flows
  - `services/*/README.md` - Individual service documentation
  - Recent `*_SUMMARY.md` and `*_AUDIT.md` files for latest changes
- This ensures context about implementation status, architectural decisions, and current priorities

## ğŸ Python Environment
- Always activate `.venv` before running Python commands: `source .venv/bin/activate`
- Python version: 3.13

## ğŸ§ª Testing
- **Unit tests**: `make test-unit` (NEVER run pytest directly)
- **Integration tests**: `make test-integration` (requires containers)
- **E2E tests**: `make test-e2e` (requires K8s cluster)
- **All tests**: `make test-all`
- **Coverage**: Automatically generated with `make test-unit` (creates coverage.xml for SonarQube)
- **Coverage requirement**: 90% minimum **for new code** to pass SonarQube quality gate
- **Always run tests before commit**
- **Test Architecture**: Follows hexagonal architecture (unit/integration/e2e pyramid)
- **Protobuf Generation**: Automatic during test execution, cleanup after
- **CI runs coverage only on unit tests**

## ğŸ³ Containers & Registry
- **Registry**: `registry.underpassai.com` (NOT localhost:5000)
- **Runtime**: Podman and CRI-O/containerd (NO Docker - it's paid software)
- **Versioning**: Tag images with semantic versions after tests pass
- **Build**: APIs are generated during container build (don't commit *_pb2.py files)
- **Image Names**: ALWAYS use fully qualified names (FQN) in Kubernetes manifests
  - âœ… `docker.io/library/neo4j:5.14` (correct for CRI-O)
  - âŒ `neo4j:5.14` (CRI-O may fail to **Coverage**: `make test-coverage` pull)
  - Reason: CRI-O doesn't default to docker.io like Docker/Podman does

## ğŸ”§ Code Quality
- **Linter**: Ruff autofix for resolving lint errors
- **Format**: Follow project conventions
- **SonarQube**: 90% coverage minimum for new code

## ğŸ“ Workflow
1. Activate .venv
2. Make changes
3. Run appropriate tests
4. Fix any lint errors with Ruff
5. Build container if needed
6. Run E2E tests if microservice changes
7. Version container image if tests pass
8. Commit with descriptive message (conventional commits)

## ğŸ—ï¸ Microservices Build Pattern
- Generate gRPC code during Docker build
- Use cache mounts for faster builds (apt, pip)
- Run as non-root user (UID 1000)
- No generated files in git (use .dockerignore)

## ğŸ”§ API Code Generation Rules - CRÃTICO

### âŒ **NEVER COMMIT Generated Code:**
- Generated gRPC code (`*_pb2.py`, `*_pb2_grpc.py`)
- Generated OpenAPI clients
- Generated TypeScript interfaces from protobuf
- Any code generated from definitions

### âœ… **ALWAYS COMMIT:**
- API definitions (`.proto` files)
- OpenAPI specifications
- Documentation and examples
- Build scripts that generate code

### ğŸ—ï¸ **Build-Time Generation Pattern:**
- Generate code during Docker build using `RUN` commands
- Store generated code in containers only
- Use `.gitignore` to exclude generated directories (`gen/`, `build/`, `dist/`)
- Generate from same definitions = consistent results

### ğŸ§ª **Testing with Generated Code:**
- **NEVER run pytest directly** - Always use `make test-unit`
- Tests automatically generate protobuf stubs in `/tmp` during execution
- Script `scripts/test/_generate_protos.sh` handles generation and cleanup
- Generated stubs are automatically cleaned up after test execution
- If tests fail with "ModuleNotFoundError: services.*.gen", use `make test-unit` instead of direct pytest

### ğŸ¯ **Why This Rule:**
1. **Avoid Merge Conflicts**: Generated code changes frequently
2. **Reproducibility**: Code can always be regenerated from definitions
3. **Clean History**: Git history focuses on actual changes, not generated noise
4. **Consistency**: All environments generate same code from same definitions
5. **Test Reliability**: Automatic generation ensures tests always have latest API definitions

### ğŸ“ **Directory Structure:**
```
services/
â”œâ”€â”€ my-service/
â”‚   â”œâ”€â”€ specs/           # âœ… API definitions
â”‚   â”‚   â””â”€â”€ my_api.proto
â”‚   â”œâ”€â”€ gen/            # âŒ Generated code (gitignored)
â”‚   â”‚   â”œâ”€â”€ my_api_pb2.py
â”‚   â”‚   â””â”€â”€ my_api_pb2_grpc.py
â”‚   â””â”€â”€ Dockerfile      # âœ… Generates code during build

scripts/test/
â”œâ”€â”€ unit.sh             # âœ… Generates stubs automatically
â”œâ”€â”€ _generate_protos.sh # âœ… Shared generation logic
â””â”€â”€ integration.sh      # âœ… Uses containers with pre-generated code
```

## âš ï¸ Common Pitfalls (Avoid These!)

### 1. Asumir estructura de API sin verificar
- **Problema**: Escribir tests asumiendo campos que no existen (ej: `response.success` cuando el proto no lo tiene)
- **SoluciÃ³n**: SIEMPRE revisar `.proto` files o cÃ³digo generado antes de escribir tests
- **Comando**: `grep "message UpdateContextResponse" specs/*.proto`

### 2. Implementar pero no integrar
- **Problema**: Crear use cases/funciones pero no llamarlas desde el punto de entrada (server.py)
- **SoluciÃ³n**: DespuÃ©s de implementar, verificar integraciÃ³n en el flujo principal
- **Check**: Si hay tests skipped, probablemente falta integraciÃ³n

### 3. Usar nombres de campos inconsistentes
- **Problema**: Dominio usa `last_status`, cÃ³digo usa `status`, tests fallan
- **SoluciÃ³n**: Revisar implementaciÃ³n del dominio ANTES de escribir tests
- **Pattern**: Read domain code â†’ Write tests matching actual fields

### 4. Ejecutar pytest directamente
- **Problema**: `pytest tests/unit/` falla con "ModuleNotFoundError: services.*.gen"
- **SoluciÃ³n**: SIEMPRE usar `make test-unit` que genera protobuf automÃ¡ticamente
- **Comando**: `make test-unit` (NO `pytest` directo)

### 5. Tests con dependencias externas en unit tests
- **Problema**: Unit tests fallan por conexiones a Redis/Neo4j/NATS
- **SoluciÃ³n**: Unit tests deben usar mocks, integration tests usan infraestructura real
- **Pattern**: Mock ports en unit tests, real adapters en integration tests

## âœ… Best Practices (Keep Doing!)

### 1. Testing exhaustivo en mÃºltiples niveles
- **Unit tests**: RÃ¡pidos (<0.3s), mocks, alta cobertura
- **Integration tests**: Con infraestructura real (testcontainers)
- **E2E tests**: Flujo completo en containers
- **Keep**: Esta pirÃ¡mide de tests nos da confianza

### 2. DocumentaciÃ³n junto con cÃ³digo
- **Roadmaps**: Documentar quÃ© falta implementar (ej: INTEGRATION_ROADMAP.md)
- **README exhaustivos**: Con ejemplos, troubleshooting, cross-references
- **Inline comments**: En cÃ³digo complejo
- **Keep**: Docs = code, no opcional

### 3. Mocks configurables y realistas
- **Pattern**: MockAgent con mÃºltiples comportamientos (EXCELLENT, POOR, STUBBORN, etc.)
- **Benefit**: Testing sin dependencias externas, casos edge cubiertos
- **Factory functions**: create_mock_council() simplifica setup
- **Keep**: Mocks = ciudadanos de primera clase, bien testeados

## ğŸ“š Documentation Philosophy - CRÃTICO

> **La documentaciÃ³n es TAN importante como el cÃ³digo. Este es un proyecto OSS de estado del arte
> que necesita financiaciÃ³n e impactar en la comunidad. La documentaciÃ³n es la primera impresiÃ³n.**

### Principios NO Negociables:

1. **Multi-Nivel**: DocumentaciÃ³n para TRES audiencias
   - **Ejecutivos/Inversores**: VisiÃ³n, impacto, diferenciaciÃ³n (INVESTORS.md, VISION.md)
   - **Desarrolladores**: Quick start, arquitectura, APIs (READMEs, architecture/)
   - **Usuarios finales**: Casos de uso, tutoriales, ejemplos (getting-started/)

2. **Meticulosidad Extrema**:
   - âœ… SIEMPRE actualizar docs cuando cambies cÃ³digo
   - âœ… VERIFICAR que referencias a archivos existen
   - âœ… CONSISTENCIA en nombres (namespace, versiones, URLs)
   - âœ… EJEMPLOS que funcionen (copy-paste ready)
   - âŒ NUNCA dejar TODOs sin resolver en docs pÃºblicos
   - âŒ NUNCA referencias a localhost, yourdomain.com, placeholders genÃ©ricos

3. **Sencilla pero Completa** (PirÃ¡mide Invertida):
   - **Primer pÃ¡rrafo**: QuÃ© hace, por quÃ© existe (30 segundos)
   - **Primera secciÃ³n**: Quick start funcional (5 minutos)
   - **Secciones siguientes**: Detalles tÃ©cnicos completos (profundidad infinita)
   - **Referencias cruzadas**: Links a docs relacionados

4. **PrecisiÃ³n TÃ©cnica**:
   - Versiones exactas (Python 3.13, NO 3.11+)
   - Comandos verificados (kubectl con namespace correcto)
   - Nombres de archivos exactos (08-context-service.yaml)
   - URLs reales (underpassai.com, NO placeholders)

5. **Mantenimiento Continuo**:
   - Hacer "due diligence" periÃ³dica (eliminar obsoletos)
   - Auditar referencias cruzadas
   - Actualizar ejemplos cuando cambien APIs
   - Eliminar documentos de branches mergeados

6. **Meticulosidad en Cambios**:
   - âœ… **ANTES de cambiar docs**: Leer TODO el contexto relacionado
   - âœ… **VERIFICAR**: Que los cambios no rompan referencias cruzadas
   - âœ… **ELIMINAR duplicaciones**: Un comando, una referencia clara
   - âœ… **CONSISTENCIA**: Mismo formato, mismo estilo en toda la doc
   - âœ… **PRECISIÃ“N**: Cada palabra cuenta, cada comando debe funcionar
   - âŒ **NUNCA**: Cambios apresurados sin revisar impacto completo
   - âŒ **NUNCA**: Dejar inconsistencias entre archivos relacionados

### Red Flags en Docs (Revisar Inmediatamente):

- ğŸš© "TODO: escribir esto mÃ¡s tarde"
- ğŸš© Referencias a archivos eliminados
- ğŸš© Comandos con namespace incorrecto
- ğŸš© Versiones inconsistentes entre docs
- ğŸš© URLs placeholder (yourdomain.com)
- ğŸš© Docs de >6 meses sin actualizar
- ğŸš© READMEs sin ejemplos funcionales
- ğŸš© **Comandos duplicados** (ej: make test-unit aparece 3 veces)
- ğŸš© **Referencias obsoletas** (ej: make test-coverage que no existe)
- ğŸš© **Inconsistencias** entre archivos (mismo tema, diferentes comandos)

### Checklist al Crear/Modificar DocumentaciÃ³n:

- [ ] Â¿Tiene quick start funcional?
- [ ] Â¿Los comandos estÃ¡n verificados?
- [ ] Â¿Las referencias a archivos existen?
- [ ] Â¿Las versiones son correctas?
- [ ] Â¿Los ejemplos copy-paste funcionan?
- [ ] Â¿Hay troubleshooting para errores comunes?
- [ ] Â¿Enlaces a documentaciÃ³n relacionada?
- [ ] Â¿Apropiado para la audiencia target?
- [ ] **Â¿He eliminado duplicaciones innecesarias?**
- [ ] **Â¿He verificado que no rompo referencias cruzadas?**
- [ ] **Â¿Los comandos son consistentes con el resto de la doc?**
- [ ] **Â¿He revisado TODOS los archivos relacionados antes de cambiar?**

## ğŸ”§ Testing Troubleshooting

### Common Test Issues & Solutions

#### "ModuleNotFoundError: services.*.gen"
- **Cause**: Protobuf stubs not generated
- **Solution**: Use `make test-unit` instead of direct pytest
- **Why**: Scripts automatically generate and cleanup protobuf stubs

#### "Connection refused" in Unit Tests
- **Cause**: Unit tests trying to connect to external services
- **Solution**: Use mocks for external dependencies
- **Pattern**: Mock ports (MessagingPort, GraphQueryPort) in unit tests

#### Tests Pass Locally but Fail in CI
- **Cause**: Missing .venv activation or different Python version
- **Solution**: Ensure CI uses same commands as local development
- **Check**: CI should run `make test-unit` not `pytest` directly

#### "No data was collected" in Coverage
- **Cause**: Coverage running on wrong modules or missing imports
- **Solution**: Use `make test-unit` which automatically sets correct coverage paths
- **Check**: Coverage should target `core` and `services` modules

#### Integration Tests Fail with "Service not found"
- **Cause**: Containers not started or services not ready
- **Solution**: Use `make test-integration` which handles container lifecycle
- **Debug**: Check `podman ps` to see running containers

### Test Execution Commands Reference

```bash
# Unit tests (fast, with mocks)
make test-unit

# Integration tests (with containers)
make test-integration

# E2E tests (with K8s cluster)
make test-e2e

# All tests
make test-all

# Coverage report (automatically generated with test-unit)
make test-unit

# NEVER run these directly:
# pytest tests/unit/          # âŒ Missing protobuf generation
# python -m pytest           # âŒ Missing .venv activation
# pytest -m integration      # âŒ Missing container setup
```




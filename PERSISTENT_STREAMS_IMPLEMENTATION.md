# ðŸ’¾ ImplementaciÃ³n de Streams y Consumers Persistentes en NATS JetStream

**Fecha**: 16 de Octubre de 2025  
**Autor**: System Architecture  
**Objetivo**: Corregir configuraciÃ³n de streams y consumers para que sean **PERSISTENTES** y **DURABLES**

---

## ðŸŽ¯ PROBLEMA IDENTIFICADO

### SÃ­ntoma
Los streams y consumers en NATS JetStream NO eran persistentes:
- **Streams** se creaban en **memory** (storage no especificado)
- **Consumers** eran **ephemeral** (sin nombre durable)
- Al reiniciar NATS pod, se perdÃ­an todos los streams y mensajes
- Consumers no recibÃ­an mensajes incluso cuando sÃ­ habÃ­a conectividad

### Causa RaÃ­z

#### 1. Streams sin Storage Persistente
```python
# ANTES (INCORRECTO):
await js.add_stream(
    name="PLANNING_EVENTS",
    subjects=["planning.>"],
    # âŒ Sin storage=FILE, defaultea a MEMORY
)
```

#### 2. Consumers sin Durable Name
```python
# ANTES (INCORRECTO):
await js.subscribe(
    "planning.plan.approved",
    queue="context-workers",  # âŒ Queue groups no funcionan en JetStream
    cb=handler,
)
```

**Problemas**:
- `queue` parameter solo funciona en **core NATS**, NO en JetStream
- Sin `durable` name, el consumer es **ephemeral** (se pierde al reiniciar pod)
- Sin `stream` parameter, JetStream no sabe de quÃ© stream leer

---

## âœ… SOLUCIÃ“N IMPLEMENTADA

### 1. Streams Persistentes con Storage FILE

**Archivo**: `services/context/streams_init.py`

```python
# DESPUÃ‰S (CORRECTO):
from nats.js.api import StorageType, RetentionPolicy, DiscardPolicy

await js.add_stream(
    name=stream_def["name"],
    subjects=stream_def["subjects"],
    storage=StorageType.FILE,  # â† CRITICAL: Persistent storage
    retention=RetentionPolicy.LIMITS,
    discard=DiscardPolicy.OLD,
    max_age=stream_def["max_age"],
    max_msgs=stream_def["max_msgs"],
    num_replicas=1,
)
logger.info(f"âœ“ Created PERSISTENT stream: {stream_def['name']} (storage: FILE)")
```

**Cambios clave**:
- `storage=StorageType.FILE` â†’ Almacena en `/data` (PersistentVolume)
- `retention=RetentionPolicy.LIMITS` â†’ Borra mensajes por tiempo/tamaÃ±o
- `discard=DiscardPolicy.OLD` â†’ Borra mensajes viejos cuando llega al lÃ­mite
- `num_replicas=1` â†’ Una sola rÃ©plica (cluster de un nodo)

**Logs mejorados**:
```python
stream_info = await js.stream_info(stream_def["name"])
logger.info(f"Stream {stream_def['name']} already exists (storage: {stream_info.config.storage})")
```

---

### 2. Consumers Durables con Nombre ExplÃ­cito

#### Context Service - Planning Consumer

**Archivo**: `services/context/consumers/planning_consumer.py`

```python
# DESPUÃ‰S (CORRECTO):
await self.js.subscribe(
    subject="planning.plan.approved",
    stream="PLANNING_EVENTS",  # â† CRITICAL: Stream name
    durable="context-planning-plan-approved",  # â† CRITICAL: Durable name
    cb=self._handle_plan_approved,
    manual_ack=True,  # Explicit acknowledgment
)
logger.info("âœ“ Subscribed to planning.plan.approved (DURABLE: context-planning-plan-approved)")
```

**Cambios clave**:
- `subject=` â†’ Nombrar el parÃ¡metro explÃ­citamente
- `stream="PLANNING_EVENTS"` â†’ Especificar de quÃ© stream leer
- `durable="context-planning-plan-approved"` â†’ Nombre Ãºnico del consumer
- `manual_ack=True` â†’ Acknowledgment explÃ­cito (mejor control de errores)

**Consumers creados en Context Service**:
1. `context-planning-story-transitions` â†’ `planning.story.transitioned`
2. `context-planning-plan-approved` â†’ `planning.plan.approved`
3. `context-orch-deliberation-completed` â†’ `orchestration.deliberation.completed`
4. `context-orch-task-dispatched` â†’ `orchestration.task.dispatched`

---

#### Context Service - Orchestration Consumer

**Archivo**: `services/context/consumers/orchestration_consumer.py`

```python
await self.js.subscribe(
    subject="orchestration.deliberation.completed",
    stream="ORCHESTRATOR_EVENTS",
    durable="context-orch-deliberation-completed",
    cb=self._handle_deliberation_completed,
    manual_ack=True,
)
```

---

#### Orchestrator Service - Planning Consumer

**Archivo**: `services/orchestrator/consumers/planning_consumer.py`

```python
await self.js.subscribe(
    subject="planning.plan.approved",
    stream="PLANNING_EVENTS",
    durable="orch-planning-plan-approved",
    cb=self._handle_plan_approved,
    manual_ack=True,
)
```

**Consumers creados en Orchestrator Service**:
1. `orch-planning-story-transitions` â†’ `planning.story.transitioned`
2. `orch-planning-plan-approved` â†’ `planning.plan.approved`
3. `orch-context-updated` â†’ `context.updated`
4. `orch-context-milestone` â†’ `context.milestone.reached`
5. `orch-context-decision` â†’ `context.decision.added`

---

#### Orchestrator Service - Context Consumer

**Archivo**: `services/orchestrator/consumers/context_consumer.py`

```python
await self.js.subscribe(
    subject="context.updated",
    stream="CONTEXT_EVENTS",
    durable="orch-context-updated",
    cb=self._handle_context_updated,
    manual_ack=True,
)
```

---

## ðŸ—ï¸ ARQUITECTURA DE PERSISTENCIA

### NATS Pod Configuration

**Archivo**: `deploy/k8s/01-nats.yaml`

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: nats
spec:
  containers:
    - name: nats
      command:
        - nats-server
        - --jetstream
        - --store_dir=/data  # â† Directorio de almacenamiento
      volumeMounts:
        - name: data
          mountPath: /data  # â† Monta PVC aquÃ­
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 10Gi  # â† PersistentVolume de 10GB
```

**CÃ³mo funciona**:
1. NATS server arranca con `--jetstream --store_dir=/data`
2. Kubernetes monta un PVC de 10GB en `/data`
3. Streams con `storage=FILE` escriben a `/data/jetstream/`
4. Al reiniciar el pod, los streams persisten porque el PVC sobrevive

---

## ðŸ“‹ TABLA DE CONSUMERS DURABLES

| Servicio | Consumer Durable | Stream | Subject | Estado |
|----------|------------------|--------|---------|--------|
| **Context** | context-planning-story-transitions | PLANNING_EVENTS | planning.story.transitioned | âœ… Implementado |
| **Context** | context-planning-plan-approved | PLANNING_EVENTS | planning.plan.approved | âœ… Implementado |
| **Context** | context-orch-deliberation-completed | ORCHESTRATOR_EVENTS | orchestration.deliberation.completed | âœ… Implementado |
| **Context** | context-orch-task-dispatched | ORCHESTRATOR_EVENTS | orchestration.task.dispatched | âœ… Implementado |
| **Orchestrator** | orch-planning-story-transitions | PLANNING_EVENTS | planning.story.transitioned | âœ… Implementado |
| **Orchestrator** | orch-planning-plan-approved | PLANNING_EVENTS | planning.plan.approved | âœ… Implementado |
| **Orchestrator** | orch-context-updated | CONTEXT_EVENTS | context.updated | âœ… Implementado |
| **Orchestrator** | orch-context-milestone | CONTEXT_EVENTS | context.milestone.reached | âœ… Implementado |
| **Orchestrator** | orch-context-decision | CONTEXT_EVENTS | context.decision.added | âœ… Implementado |

**Total**: 9 consumers durables creados

---

## ðŸ”§ BENEFICIOS DE LA SOLUCIÃ“N

### 1. Persistencia Completa âœ…
- **Streams sobreviven** a reinicios de NATS pod
- **Mensajes persisten** en disco (no se pierden)
- **Consumers sobreviven** a reinicios de servicios
- **Offset tracking** automÃ¡tico (no procesa mensajes duplicados)

### 2. GarantÃ­as de Entrega âœ…
- **At-least-once delivery**: Con `manual_ack=True`
- **No message loss**: Gracias a storage FILE
- **Retry automÃ¡tico**: Si un handler falla, mensaje se re-entrega

### 3. Observabilidad Mejorada âœ…
- Logs claros de creaciÃ³n: `"DURABLE: context-planning-plan-approved"`
- InspecciÃ³n de storage type: `stream_info.config.storage`
- Error logging mejorado: `exc_info=True` en excepciones

### 4. Escalabilidad âœ…
- MÃºltiples pods pueden compartir el mismo durable consumer (load balancing)
- Cada pod procesa mensajes diferentes del mismo stream
- No hay duplicaciÃ³n de procesamiento

---

## ðŸ§ª TESTING Y VERIFICACIÃ“N

### Comando 1: Ver Streams Creados
```bash
kubectl exec -n swe-ai-fleet nats-0 -- nats stream ls
```

**Esperado**:
```
PLANNING_EVENTS (storage: file)
CONTEXT_EVENTS (storage: file)
ORCHESTRATOR_EVENTS (storage: file)
AGENT_COMMANDS (storage: file)
AGENT_RESPONSES (storage: file)
```

### Comando 2: Ver Consumers de un Stream
```bash
kubectl exec -n swe-ai-fleet nats-0 -- nats consumer ls PLANNING_EVENTS
```

**Esperado**:
```
context-planning-story-transitions
context-planning-plan-approved
orch-planning-story-transitions
orch-planning-plan-approved
```

### Comando 3: Inspeccionar Consumer
```bash
kubectl exec -n swe-ai-fleet nats-0 -- nats consumer info PLANNING_EVENTS context-planning-plan-approved
```

**Esperado**:
```
Durable Name: context-planning-plan-approved
Ack Policy: Explicit
Replay Policy: Instant
Deliver Policy: All
Pending Messages: X
Redelivered: 0
```

### Comando 4: Ver Mensajes en Stream
```bash
kubectl exec -n swe-ai-fleet nats-0 -- nats stream view PLANNING_EVENTS --last=10
```

### Comando 5: Test de Persistencia
```bash
# 1. Publicar mensaje
kubectl run nats-pub --rm -i --restart=Never \
  --image=docker.io/natsio/nats-box:latest \
  -n swe-ai-fleet -- \
  nats pub planning.plan.approved '{"test":"data"}'

# 2. Ver que el stream tiene el mensaje
kubectl exec -n swe-ai-fleet nats-0 -- nats stream info PLANNING_EVENTS

# 3. Reiniciar NATS
kubectl delete pod nats-0 -n swe-ai-fleet

# 4. Verificar que el mensaje AÃšN EXISTE
kubectl exec -n swe-ai-fleet nats-0 -- nats stream info PLANNING_EVENTS
```

---

## ðŸ“ CAMBIOS EN ARCHIVOS

### Archivos Modificados

1. âœ… `services/context/streams_init.py`
   - Streams con `storage=StorageType.FILE`
   - Logging mejorado con storage type

2. âœ… `services/context/consumers/planning_consumer.py`
   - Consumers durables: `context-planning-*`
   - `manual_ack=True`

3. âœ… `services/context/consumers/orchestration_consumer.py`
   - Consumers durables: `context-orch-*`
   - `manual_ack=True`

4. âœ… `services/orchestrator/consumers/planning_consumer.py`
   - Consumers durables: `orch-planning-*`
   - `manual_ack=True`

5. âœ… `services/orchestrator/consumers/context_consumer.py`
   - Consumers durables: `orch-context-*`
   - `manual_ack=True`

### Archivos No Modificados (Ya OK)

- âœ… `deploy/k8s/01-nats.yaml` - Ya tenÃ­a PVC configurado correctamente
- âœ… `deploy/k8s/00-configmaps.yaml` - NATS_URL con FQN correcto

---

## ðŸš€ PRÃ“XIMOS PASOS

### 1. Rebuild & Redeploy Services (CRÃTICO)

**Context Service**:
```bash
cd services/context
# Rebuild container con cambios
docker build -t registry.underpassai.com/swe-fleet/context:v0.7.0 .
docker push registry.underpassai.com/swe-fleet/context:v0.7.0

# Update deployment
kubectl set image deployment/context context=registry.underpassai.com/swe-fleet/context:v0.7.0 -n swe-ai-fleet
```

**Orchestrator Service**:
```bash
cd services/orchestrator
# Rebuild container con cambios
docker build -t registry.underpassai.com/swe-fleet/orchestrator:v0.6.0 .
docker push registry.underpassai.com/swe-fleet/orchestrator:v0.6.0

# Update deployment
kubectl set image deployment/orchestrator orchestrator=registry.underpassai.com/swe-fleet/orchestrator:v0.6.0 -n swe-ai-fleet
```

### 2. Reiniciar NATS (Recrear Streams)

```bash
# Eliminar streams viejos (memory)
kubectl delete pod nats-0 -n swe-ai-fleet

# Los nuevos pods de Context/Orchestrator crearÃ¡n streams FILE
kubectl logs -n swe-ai-fleet deployment/context --tail=50 | grep "stream"
# Esperado: "âœ“ Created PERSISTENT stream: PLANNING_EVENTS (storage: FILE)"
```

### 3. Verificar Consumers Creados

```bash
# Esperar 30 segundos a que arranquen
sleep 30

# Ver consumers
kubectl exec -n swe-ai-fleet nats-0 -- nats consumer ls PLANNING_EVENTS
kubectl exec -n swe-ai-fleet nats-0 -- nats consumer ls CONTEXT_EVENTS
kubectl exec -n swe-ai-fleet nats-0 -- nats consumer ls ORCHESTRATOR_EVENTS
```

### 4. Test End-to-End

```bash
# Publicar evento de prueba
kubectl run nats-test --rm -i --restart=Never \
  --image=docker.io/natsio/nats-box:latest \
  -n swe-ai-fleet -- \
  nats pub planning.plan.approved '{
    "story_id": "US-TEST-001",
    "plan_id": "plan-test",
    "timestamp": "2025-10-16T18:00:00Z"
  }'

# Verificar logs de consumers (DEBE aparecer "Plan approved:")
kubectl logs -n swe-ai-fleet deployment/context --tail=20 | grep "Plan approved"
kubectl logs -n swe-ai-fleet deployment/orchestrator --tail=20 | grep "Plan approved"

# Verificar Neo4j
kubectl exec -n swe-ai-fleet neo4j-0 -- cypher-shell -u neo4j -p testpassword \
  "MATCH (n:PlanApproval) WHERE n.story_id = 'US-TEST-001' RETURN n;"
# Esperado: 1 nodo
```

---

## âš ï¸ IMPORTANTE - BREAKING CHANGES

### Cambio en API de nats-py

La API de `js.subscribe()` puede variar dependiendo de la versiÃ³n de `nats-py`.

**VersiÃ³n requerida**: `nats-py >= 2.6.0`

```python
# Verificar versiÃ³n en requirements.txt o Pipfile
nats-py==2.6.0
```

Si usas versiÃ³n anterior, la sintaxis puede ser:
```python
# nats-py < 2.0
await js.subscribe("subject", durable="name", cb=handler)

# nats-py >= 2.6
await js.subscribe(subject="subject", durable="name", stream="STREAM", cb=handler)
```

---

## ðŸ“š REFERENCIAS

- [NATS JetStream Persistence](https://docs.nats.io/nats-concepts/jetstream/streams#storage)
- [NATS JetStream Consumers](https://docs.nats.io/nats-concepts/jetstream/consumers)
- [nats-py Documentation](https://github.com/nats-io/nats.py)
- [COMMUNICATION_AUDIT.md](./COMMUNICATION_AUDIT.md) - Arquitectura de comunicaciones
- [VERIFICATION_RESULTS.md](./VERIFICATION_RESULTS.md) - Problema identificado

---

**Resumen**: Streams ahora son **FILE-based** (persisten en PVC) y consumers ahora son **DURABLE** (sobreviven a reinicios). Esto corrige el problema crÃ­tico de que los eventos no se estaban procesando.

**PrÃ³ximo Deploy**: Rebuild Context v0.7.0 + Orchestrator v0.6.0 â†’ Test E2E â†’ Verificar persistencia

